[
  {
    "objectID": "researches.html",
    "href": "researches.html",
    "title": "Researches",
    "section": "",
    "text": "We have been developing tensor factorization and deep learning methods to facilitate the analysis of epigenomic data and use integrative modeling approaches to study genomic transcriptional and epigenetic gene regulatory mechanisms underlying psychiatry diseases. We are developing new methods to utilize the abundant public relationship data to understand targets and specificity of drugs in treating Alzheimer’s Disease."
  },
  {
    "objectID": "researches.html#machine-learning-frameworks-in-biomedical-science",
    "href": "researches.html#machine-learning-frameworks-in-biomedical-science",
    "title": "Researches",
    "section": "",
    "text": "We have been developing tensor factorization and deep learning methods to facilitate the analysis of epigenomic data and use integrative modeling approaches to study genomic transcriptional and epigenetic gene regulatory mechanisms underlying psychiatry diseases. We are developing new methods to utilize the abundant public relationship data to understand targets and specificity of drugs in treating Alzheimer’s Disease."
  },
  {
    "objectID": "researches.html#statistical-genetics-for-causal-relationships",
    "href": "researches.html#statistical-genetics-for-causal-relationships",
    "title": "Researches",
    "section": "Statistical Genetics for Causal Relationships",
    "text": "Statistical Genetics for Causal Relationships\n\n\nWe have been utilizing causal inference methods to facilitate the analysis of genotyping data and employing Mendelian randomization approaches to investigate the causal associations between Alzheimer’s disease and various drugs, viruses, and other risk factors. Our investigations on aspirin and GLP-1R agonists suggest that they may have the potential to offer effective neuroprotective properties."
  },
  {
    "objectID": "researches.html#target-trial-emulation",
    "href": "researches.html#target-trial-emulation",
    "title": "Researches",
    "section": "Target trial emulation",
    "text": "Target trial emulation\n\n\nWe are developing deep learning-based methods to approximate the results of a randomized trial by directly comparing outcomes between individuals who received the treatment of interest and those who did not. Additionally, by analyzing electronic health records, we are identifying potential drug combinations for controlling blood pressure and exploring novel drugs for neurodegenerative diseases."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pingjian DING",
    "section": "",
    "text": "Pingjian DING currently serves as an Associate Professor at the University of South China. Prior to joining the University of South China, he was a Postdoctoral Scholar and later promoted to a research associate at the School of Medicine, Case Western Reserve University, collaborating with Dr. Rong XU. He earned his Ph.D. in Computer Science from the College of Computer Science and Electronic Engineering at Hunan University in 2019, supervised by Dr. Jiawei LUO. From August 2017 to September 2018, he was a visiting Ph.D. student at the biomedical informatics lab of Nanyang Technological University, supervised by Dr. Chee-Keong KWOH. His research primarily focuses on integrating machine learning and deep learning into biostatistical methodologies, particularly in neurological research, with a focus on addressing Alzheimer’s Disease.\nFind What You Love and Let It Kill You!"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "dpjhnu@qq.com\ndpjhnu@gmail.com\ndpj@usc.edu.cn"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs.html#machine-learning",
    "href": "blogs.html#machine-learning",
    "title": "Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs.html#statistics",
    "href": "blogs.html#statistics",
    "title": "Blogs",
    "section": "Statistics",
    "text": "Statistics\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nTechniques to adjust for confounding in observational studies\n\n\nConfounding\n\n\nNov 17, 2023\n\n\n\n\nUnderstanding Instrumental Variables\n\n\nIVs\n\n\nNov 16, 2023\n\n\n\n\nConfounding Bias\n\n\nBias\n\n\nNov 14, 2023\n\n\n\n\nSelection Bias\n\n\nBias\n\n\nNov 14, 2023\n\n\n\n\nCollider bias\n\n\nBias\n\n\nNov 14, 2023\n\n\n\n\nWhat Are Degrees of Freedom in Statistics?\n\n\nDegrees of Freedom\n\n\nNov 11, 2023\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs.html#bioinformatics",
    "href": "blogs.html#bioinformatics",
    "title": "Blogs",
    "section": "Bioinformatics",
    "text": "Bioinformatics\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nSubtitle\n\n\nDate\n\n\n\n\n\n\nRNA Sequencing\n\n\nRNA, Sequencing\n\n\nNov 30, 2023\n\n\n\n\nDNA Sequencing\n\n\nGenotyping, Sequencing\n\n\nNov 18, 2023\n\n\n\n\nWhat is Epigenetics?\n\n\nEpigenetics\n\n\nNov 18, 2023\n\n\n\n\nMR Using Genetics to Study Behaviors and Environments that Cause Disease\n\n\nMR\n\n\nNov 12, 2023\n\n\n\n\nConversion between chr:pos and rsID\n\n\nrsID, SNP, (in Chinese)\n\n\nNov 6, 2023\n\n\n\n\nRunning local LD operations using ieugwasr\n\n\nR, LD\n\n\nOct 31, 2023\n\n\n\n\nCalculate Polygenic Risk Score on UKB\n\n\nPolygenic Risk Score, UKB\n\n\nOct 27, 2023\n\n\n\n\nUse Jupyter Spark to Extract Phenotypic Data from the UKB Database\n\n\nJupyter, UKB\n\n\nOct 27, 2023\n\n\n\n\ndx extract_dataset (UKB) for R\n\n\nR, UKB\n\n\nOct 27, 2023\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/tools/09-tools.html",
    "href": "blogs/tools/09-tools.html",
    "title": "How to Plot Figures from My Previous Articles (R)",
    "section": "",
    "text": "plot_fig1.knit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nload the data\nrm(list = ls())\nlibrary(forestplot)\nlibrary(dplyr)\nlibrary(pheatmap)\nlibrary(RColorBrewer)\n\n# '20230515_covid_regional_contex.csv' or '20230515_covid_left_right_subcortex.csv'\nfile_path &lt;- '20230515_covid_left_right_subcortex.csv' \nmeasure_label &lt;- 'volumn' # 'thickavg' or 'surfavg' or 'volumn'\n\ndata &lt;- read.csv(file_path, header = TRUE)\nhead(data[c('exposure', 'outcome', 'method', 'b', 'se.x', 'pval.x')])\n##          exposure             outcome                    method            b\n## 1 Covid infection          Brain.stem Inverse variance weighted  0.003790495\n## 2 Covid infection          Brain.stem                  MR Egger -0.017836714\n## 3 Covid infection          Brain.stem           Weighted median -0.012797067\n## 4 Covid infection left.accumbens.area Inverse variance weighted  0.028352371\n## 5 Covid infection left.accumbens.area                  MR Egger  0.036179417\n## 6 Covid infection left.accumbens.area           Weighted median  0.029585339\n##         se.x    pval.x\n## 1 0.04553056 0.9336514\n## 2 0.10800493 0.8768384\n## 3 0.05361871 0.8113631\n## 4 0.05489680 0.6055285\n## 5 0.12231116 0.7821046\n## 6 0.06515554 0.6497766\nprocess data\ndata_infection &lt;- data |&gt; filter(exposure=='Covid infection')\ndata_severity &lt;- data |&gt; filter(exposure=='Covid severity')\n\ndata_infection &lt;- data_infection |&gt;\n  filter(measurement == measure_label) |&gt;   #'thickness', 'surface area'\n  filter(method == 'Inverse variance weighted')\n\ndata_severity &lt;- data_severity |&gt;\n  filter(measurement == measure_label) |&gt;   #'thickness', 'surface area'\n  filter(method == 'Inverse variance weighted')\n\ndata_plot &lt;- data.frame(row.names = data_severity$outcome)\ndata_plot$infection &lt;- data_infection$b/data_infection$se.x\ndata_plot$severity &lt;- data_severity$b/data_severity$se.x\n\ndata_p &lt;- data.frame(row.names = data_severity$outcome)\ndata_p$infection &lt;- data_infection$pval.x\ndata_p$severity &lt;- data_severity$pval.x\n\nhead(data_p)\n##                     infection  severity\n## Brain.stem          0.9336514 0.9055478\n## left.accumbens.area 0.6055285 0.8713543\n## left.amygdala       0.4501532 0.8772266\n## left.caudate        0.6982444 0.5947670\n## left.hippocampus    0.9169623 0.6035164\n## left.insula         0.7867675 0.5811359\nplot the figure\nbk &lt;- c(seq(-1.5,-0.01,by=0.01),seq(0,1.5,by=0.01))\npheatmap(as.data.frame(t(data_plot)),\n         breaks = bk,\n         legend_breaks=seq(-1.5,1.5,0.5),\n         legend_labels = c(\"-1.5\",\"-1\",\"-0.5\",\"0\",\"0.5\",\"1\",\"1.5\"),\n         color = colorRampPalette(rev(brewer.pal(n = 7, \n                                                 name = \"RdYlBu\")))(length(bk)),\n         cluster_cols = FALSE,\n         cluster_rows = FALSE,\n         #display_numbers = matrix(ifelse(data_p &lt; 0.05, \"*\", \"\"), nrow(data_p)),\n         fontsize_number = 12,\n         fontsize_row = 12,\n         angle_col = \"315\",\n         #annotation_colors = \"white\",\n         number_color = \"black\",\n         name = \"Z-Score\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_fig2.knit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nload data\nrm(list = ls())\nlibrary(ComplexHeatmap)\nlibrary(circlize)\nlibrary(RColorBrewer)\n\ndata_global &lt;- read.csv('20230515_dna_covid_global_cortex.csv') \ndata_global &lt;- data_global[data_global$method == 'Inverse variance weighted',]\ndata_global$group &lt;- 'Global'\n\ndata_regional &lt;- read.csv('20230515_dna_covid_regional_contex.csv') \ndata_regional &lt;- data_regional[data_regional$method == 'Inverse variance weighted',]\ndata_regional$group &lt;- 'Regional'\n\ndata_left_right &lt;- read.csv('20230515_dna_covid_left_right_subcortex.csv') \ndata_left_right &lt;- data_left_right[data_left_right$method == 'Inverse variance weighted',]\ndata_left_right$group &lt;- 'Left Right'\n\ndata_combined &lt;- read.csv('20230515_dna_covid_combined_subcortex.csv') \ndata_combined &lt;- data_combined[data_combined$method == 'Inverse variance weighted',]\ndata_combined$group &lt;- 'Combined'\nprocess data\ndata_combined &lt;- data_combined[c('outcome','exposure','b','se', 'group')]\ndata_combined$Z_score &lt;- data_combined$b/data_combined$se\ndata_global &lt;- data_global[c('outcome','exposure','b','se', 'group')]\ndata_global$Z_score &lt;- data_global$b/data_global$se\ndata_regional &lt;- data_regional[c('outcome','exposure','b','se', 'group')]\ndata_regional$Z_score &lt;- data_regional$b/data_regional$se\ndata_left_right &lt;- data_left_right[c('outcome','exposure','b','se', 'group')]\ndata_left_right$Z_score &lt;- data_left_right$b/data_left_right$se\n\nexposure_list &lt;- list(\"COVID-19 positive vs COVID-19 negative or COVID-19 status unknown\",\n                      \"COVID-19 positive not hospitalized vs COVID-19 negative or COVID-19 status unknown\",\n                      \"COVID-19 positive vs COVID-19 negative\",\n                      \"COVID-19 positive hospitalized vs COVID-19 positive not hospitalized\",\n                      \"COVID-19 positive hospitalized vs COVID-19 negative or COVID-19 status unknown\",\n                      \"COVID-19 positive severe vs COVID-19 negative or COVID-19 status unknown\")\n\nexposure1 &lt;- exposure_list[1]\nglobal_infection1 &lt;- data_global[data_global$exposure == exposure1,]\nregional_infection1 &lt;- data_regional[data_regional$exposure == exposure1,]\ncombined_infection1 &lt;- data_combined[data_combined$exposure == exposure1,]\nleft_right_infection1 &lt;- data_left_right[data_left_right$exposure == exposure1,]\ndat_inf1 &lt;- rbind(global_infection1,regional_infection1,combined_infection1,left_right_infection1)\ndat_infection1 &lt;- data.frame(row.names = dat_inf1$outcome,\n                             dat_inf1[c('Z_score','group')])\ncolnames(dat_infection1) &lt;- c(exposure1,'group')\n\ndat_all &lt;- data.frame(row.names = dat_infection1)\n\nfor (exposure1 in exposure_list) {\n  global_infection1 &lt;- data_global[data_global$exposure == exposure1,]\n  regional_infection1 &lt;- data_regional[data_regional$exposure == exposure1,]\n  combined_infection1 &lt;- data_combined[data_combined$exposure == exposure1,]\n  left_right_infection1 &lt;- data_left_right[data_left_right$exposure == exposure1,]\n  dat_1 &lt;- rbind(global_infection1,regional_infection1,combined_infection1,left_right_infection1)\n  dat_1 &lt;- data.frame(row.names = dat_1$outcome, \n                      dat_1$Z_score)\n  colnames(dat_1) &lt;- exposure1\n  dat_all &lt;- cbind(dat_all, dat_1)\n}\n\nsplit &lt;- factor(dat_infection1$group, levels = c(\"Global\", \"Regional\", \"Left Right\", \"Combined\"))\n\ndat_all &lt;- as.matrix(dat_all)\n\nhead(dat_all)\n##                           COVID-19 positive vs COVID-19 negative or COVID-19 status unknown\n## global cortical.s                                                                 0.3744224\n## global cortical.t                                                                -1.9084616\n## bankssts.s                                                                       -0.4940221\n## caudalanteriorcingulate.s                                                        -1.6635093\n## caudalmiddlefrontal.s                                                             1.8913544\n## cuneus.s                                                                          1.0744033\n##                           COVID-19 positive not hospitalized vs COVID-19 negative or COVID-19 status unknown\n## global cortical.s                                                                                 -0.3374852\n## global cortical.t                                                                                 -0.8519807\n## bankssts.s                                                                                        -0.7780077\n## caudalanteriorcingulate.s                                                                         -1.0929856\n## caudalmiddlefrontal.s                                                                              1.0189359\n## cuneus.s                                                                                           1.0387681\n##                           COVID-19 positive vs COVID-19 negative\n## global cortical.s                                     -0.7255839\n## global cortical.t                                      1.1139845\n## bankssts.s                                            -0.6261338\n## caudalanteriorcingulate.s                             -0.8362567\n## caudalmiddlefrontal.s                                  0.2208114\n## cuneus.s                                               1.3282123\n##                           COVID-19 positive hospitalized vs COVID-19 positive not hospitalized\n## global cortical.s                                                                    0.2266332\n## global cortical.t                                                                    0.0364409\n## bankssts.s                                                                          -0.4992253\n## caudalanteriorcingulate.s                                                            0.2770829\n## caudalmiddlefrontal.s                                                                1.0426460\n## cuneus.s                                                                             0.4812868\n##                           COVID-19 positive hospitalized vs COVID-19 negative or COVID-19 status unknown\n## global cortical.s                                                                           -0.916836029\n## global cortical.t                                                                            0.263431181\n## bankssts.s                                                                                  -1.331328284\n## caudalanteriorcingulate.s                                                                   -1.269418419\n## caudalmiddlefrontal.s                                                                        0.215746987\n## cuneus.s                                                                                     0.004246477\n##                           COVID-19 positive severe vs COVID-19 negative or COVID-19 status unknown\n## global cortical.s                                                                      -0.84124152\n## global cortical.t                                                                       0.12259122\n## bankssts.s                                                                              0.60898292\n## caudalanteriorcingulate.s                                                              -0.06954886\n## caudalmiddlefrontal.s                                                                  -1.45286052\n## cuneus.s                                                                               -2.30837430\nplot figure\ncircos.clear()\n\ncolor = colorRampPalette(rev(brewer.pal(n = 7, \n                                        name = \"RdYlBu\")))(401)\ncol_fun1 = colorRamp2(seq(-2,2,0.01), color)\ncircos.par(start.degree = 90, gap.degree = 10)\n\ncircos.heatmap(as.matrix(dat_all[,1]), split = split, col = col_fun1, track.height = 0.05,\n               show.sector.labels = TRUE)\ncircos.heatmap(as.matrix(dat_all[,2]), split = split, col = col_fun1, track.height = 0.05,\n               show.sector.labels = TRUE)\ncircos.heatmap(as.matrix(dat_all[,3]), split = split, col = col_fun1, track.height = 0.05,\n               rownames.side = \"inside\",\n               show.sector.labels = TRUE)\n\ncircos.heatmap(as.matrix(dat_all[,4]), split = split, col = col_fun1, track.height = 0.05,\n               show.sector.labels = TRUE)\ncircos.heatmap(as.matrix(dat_all[,5]), split = split, col = col_fun1, track.height = 0.05,\n               show.sector.labels = TRUE)\ncircos.heatmap(as.matrix(dat_all[,6]), split = split, col = col_fun1, track.height = 0.05,\n               show.sector.labels = TRUE)\n\n# add legend\nlgd = Legend(title = \"Z Score\", col_fun = col_fun1)\ngrid.draw(lgd)"
  },
  {
    "objectID": "blogs/tools/09-tools.html#heatmap",
    "href": "blogs/tools/09-tools.html#heatmap",
    "title": "How to Plot Figures from My Previous Articles (R)",
    "section": "",
    "text": "plot_fig1.knit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nload the data\nrm(list = ls())\nlibrary(forestplot)\nlibrary(dplyr)\nlibrary(pheatmap)\nlibrary(RColorBrewer)\n\n# '20230515_covid_regional_contex.csv' or '20230515_covid_left_right_subcortex.csv'\nfile_path &lt;- '20230515_covid_left_right_subcortex.csv' \nmeasure_label &lt;- 'volumn' # 'thickavg' or 'surfavg' or 'volumn'\n\ndata &lt;- read.csv(file_path, header = TRUE)\nhead(data[c('exposure', 'outcome', 'method', 'b', 'se.x', 'pval.x')])\n##          exposure             outcome                    method            b\n## 1 Covid infection          Brain.stem Inverse variance weighted  0.003790495\n## 2 Covid infection          Brain.stem                  MR Egger -0.017836714\n## 3 Covid infection          Brain.stem           Weighted median -0.012797067\n## 4 Covid infection left.accumbens.area Inverse variance weighted  0.028352371\n## 5 Covid infection left.accumbens.area                  MR Egger  0.036179417\n## 6 Covid infection left.accumbens.area           Weighted median  0.029585339\n##         se.x    pval.x\n## 1 0.04553056 0.9336514\n## 2 0.10800493 0.8768384\n## 3 0.05361871 0.8113631\n## 4 0.05489680 0.6055285\n## 5 0.12231116 0.7821046\n## 6 0.06515554 0.6497766\nprocess data\ndata_infection &lt;- data |&gt; filter(exposure=='Covid infection')\ndata_severity &lt;- data |&gt; filter(exposure=='Covid severity')\n\ndata_infection &lt;- data_infection |&gt;\n  filter(measurement == measure_label) |&gt;   #'thickness', 'surface area'\n  filter(method == 'Inverse variance weighted')\n\ndata_severity &lt;- data_severity |&gt;\n  filter(measurement == measure_label) |&gt;   #'thickness', 'surface area'\n  filter(method == 'Inverse variance weighted')\n\ndata_plot &lt;- data.frame(row.names = data_severity$outcome)\ndata_plot$infection &lt;- data_infection$b/data_infection$se.x\ndata_plot$severity &lt;- data_severity$b/data_severity$se.x\n\ndata_p &lt;- data.frame(row.names = data_severity$outcome)\ndata_p$infection &lt;- data_infection$pval.x\ndata_p$severity &lt;- data_severity$pval.x\n\nhead(data_p)\n##                     infection  severity\n## Brain.stem          0.9336514 0.9055478\n## left.accumbens.area 0.6055285 0.8713543\n## left.amygdala       0.4501532 0.8772266\n## left.caudate        0.6982444 0.5947670\n## left.hippocampus    0.9169623 0.6035164\n## left.insula         0.7867675 0.5811359\nplot the figure\nbk &lt;- c(seq(-1.5,-0.01,by=0.01),seq(0,1.5,by=0.01))\npheatmap(as.data.frame(t(data_plot)),\n         breaks = bk,\n         legend_breaks=seq(-1.5,1.5,0.5),\n         legend_labels = c(\"-1.5\",\"-1\",\"-0.5\",\"0\",\"0.5\",\"1\",\"1.5\"),\n         color = colorRampPalette(rev(brewer.pal(n = 7, \n                                                 name = \"RdYlBu\")))(length(bk)),\n         cluster_cols = FALSE,\n         cluster_rows = FALSE,\n         #display_numbers = matrix(ifelse(data_p &lt; 0.05, \"*\", \"\"), nrow(data_p)),\n         fontsize_number = 12,\n         fontsize_row = 12,\n         angle_col = \"315\",\n         #annotation_colors = \"white\",\n         number_color = \"black\",\n         name = \"Z-Score\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_fig2.knit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nload data\nrm(list = ls())\nlibrary(ComplexHeatmap)\nlibrary(circlize)\nlibrary(RColorBrewer)\n\ndata_global &lt;- read.csv('20230515_dna_covid_global_cortex.csv') \ndata_global &lt;- data_global[data_global$method == 'Inverse variance weighted',]\ndata_global$group &lt;- 'Global'\n\ndata_regional &lt;- read.csv('20230515_dna_covid_regional_contex.csv') \ndata_regional &lt;- data_regional[data_regional$method == 'Inverse variance weighted',]\ndata_regional$group &lt;- 'Regional'\n\ndata_left_right &lt;- read.csv('20230515_dna_covid_left_right_subcortex.csv') \ndata_left_right &lt;- data_left_right[data_left_right$method == 'Inverse variance weighted',]\ndata_left_right$group &lt;- 'Left Right'\n\ndata_combined &lt;- read.csv('20230515_dna_covid_combined_subcortex.csv') \ndata_combined &lt;- data_combined[data_combined$method == 'Inverse variance weighted',]\ndata_combined$group &lt;- 'Combined'\nprocess data\ndata_combined &lt;- data_combined[c('outcome','exposure','b','se', 'group')]\ndata_combined$Z_score &lt;- data_combined$b/data_combined$se\ndata_global &lt;- data_global[c('outcome','exposure','b','se', 'group')]\ndata_global$Z_score &lt;- data_global$b/data_global$se\ndata_regional &lt;- data_regional[c('outcome','exposure','b','se', 'group')]\ndata_regional$Z_score &lt;- data_regional$b/data_regional$se\ndata_left_right &lt;- data_left_right[c('outcome','exposure','b','se', 'group')]\ndata_left_right$Z_score &lt;- data_left_right$b/data_left_right$se\n\nexposure_list &lt;- list(\"COVID-19 positive vs COVID-19 negative or COVID-19 status unknown\",\n                      \"COVID-19 positive not hospitalized vs COVID-19 negative or COVID-19 status unknown\",\n                      \"COVID-19 positive vs COVID-19 negative\",\n                      \"COVID-19 positive hospitalized vs COVID-19 positive not hospitalized\",\n                      \"COVID-19 positive hospitalized vs COVID-19 negative or COVID-19 status unknown\",\n                      \"COVID-19 positive severe vs COVID-19 negative or COVID-19 status unknown\")\n\nexposure1 &lt;- exposure_list[1]\nglobal_infection1 &lt;- data_global[data_global$exposure == exposure1,]\nregional_infection1 &lt;- data_regional[data_regional$exposure == exposure1,]\ncombined_infection1 &lt;- data_combined[data_combined$exposure == exposure1,]\nleft_right_infection1 &lt;- data_left_right[data_left_right$exposure == exposure1,]\ndat_inf1 &lt;- rbind(global_infection1,regional_infection1,combined_infection1,left_right_infection1)\ndat_infection1 &lt;- data.frame(row.names = dat_inf1$outcome,\n                             dat_inf1[c('Z_score','group')])\ncolnames(dat_infection1) &lt;- c(exposure1,'group')\n\ndat_all &lt;- data.frame(row.names = dat_infection1)\n\nfor (exposure1 in exposure_list) {\n  global_infection1 &lt;- data_global[data_global$exposure == exposure1,]\n  regional_infection1 &lt;- data_regional[data_regional$exposure == exposure1,]\n  combined_infection1 &lt;- data_combined[data_combined$exposure == exposure1,]\n  left_right_infection1 &lt;- data_left_right[data_left_right$exposure == exposure1,]\n  dat_1 &lt;- rbind(global_infection1,regional_infection1,combined_infection1,left_right_infection1)\n  dat_1 &lt;- data.frame(row.names = dat_1$outcome, \n                      dat_1$Z_score)\n  colnames(dat_1) &lt;- exposure1\n  dat_all &lt;- cbind(dat_all, dat_1)\n}\n\nsplit &lt;- factor(dat_infection1$group, levels = c(\"Global\", \"Regional\", \"Left Right\", \"Combined\"))\n\ndat_all &lt;- as.matrix(dat_all)\n\nhead(dat_all)\n##                           COVID-19 positive vs COVID-19 negative or COVID-19 status unknown\n## global cortical.s                                                                 0.3744224\n## global cortical.t                                                                -1.9084616\n## bankssts.s                                                                       -0.4940221\n## caudalanteriorcingulate.s                                                        -1.6635093\n## caudalmiddlefrontal.s                                                             1.8913544\n## cuneus.s                                                                          1.0744033\n##                           COVID-19 positive not hospitalized vs COVID-19 negative or COVID-19 status unknown\n## global cortical.s                                                                                 -0.3374852\n## global cortical.t                                                                                 -0.8519807\n## bankssts.s                                                                                        -0.7780077\n## caudalanteriorcingulate.s                                                                         -1.0929856\n## caudalmiddlefrontal.s                                                                              1.0189359\n## cuneus.s                                                                                           1.0387681\n##                           COVID-19 positive vs COVID-19 negative\n## global cortical.s                                     -0.7255839\n## global cortical.t                                      1.1139845\n## bankssts.s                                            -0.6261338\n## caudalanteriorcingulate.s                             -0.8362567\n## caudalmiddlefrontal.s                                  0.2208114\n## cuneus.s                                               1.3282123\n##                           COVID-19 positive hospitalized vs COVID-19 positive not hospitalized\n## global cortical.s                                                                    0.2266332\n## global cortical.t                                                                    0.0364409\n## bankssts.s                                                                          -0.4992253\n## caudalanteriorcingulate.s                                                            0.2770829\n## caudalmiddlefrontal.s                                                                1.0426460\n## cuneus.s                                                                             0.4812868\n##                           COVID-19 positive hospitalized vs COVID-19 negative or COVID-19 status unknown\n## global cortical.s                                                                           -0.916836029\n## global cortical.t                                                                            0.263431181\n## bankssts.s                                                                                  -1.331328284\n## caudalanteriorcingulate.s                                                                   -1.269418419\n## caudalmiddlefrontal.s                                                                        0.215746987\n## cuneus.s                                                                                     0.004246477\n##                           COVID-19 positive severe vs COVID-19 negative or COVID-19 status unknown\n## global cortical.s                                                                      -0.84124152\n## global cortical.t                                                                       0.12259122\n## bankssts.s                                                                              0.60898292\n## caudalanteriorcingulate.s                                                              -0.06954886\n## caudalmiddlefrontal.s                                                                  -1.45286052\n## cuneus.s                                                                               -2.30837430\nplot figure\ncircos.clear()\n\ncolor = colorRampPalette(rev(brewer.pal(n = 7, \n                                        name = \"RdYlBu\")))(401)\ncol_fun1 = colorRamp2(seq(-2,2,0.01), color)\ncircos.par(start.degree = 90, gap.degree = 10)\n\ncircos.heatmap(as.matrix(dat_all[,1]), split = split, col = col_fun1, track.height = 0.05,\n               show.sector.labels = TRUE)\ncircos.heatmap(as.matrix(dat_all[,2]), split = split, col = col_fun1, track.height = 0.05,\n               show.sector.labels = TRUE)\ncircos.heatmap(as.matrix(dat_all[,3]), split = split, col = col_fun1, track.height = 0.05,\n               rownames.side = \"inside\",\n               show.sector.labels = TRUE)\n\ncircos.heatmap(as.matrix(dat_all[,4]), split = split, col = col_fun1, track.height = 0.05,\n               show.sector.labels = TRUE)\ncircos.heatmap(as.matrix(dat_all[,5]), split = split, col = col_fun1, track.height = 0.05,\n               show.sector.labels = TRUE)\ncircos.heatmap(as.matrix(dat_all[,6]), split = split, col = col_fun1, track.height = 0.05,\n               show.sector.labels = TRUE)\n\n# add legend\nlgd = Legend(title = \"Z Score\", col_fun = col_fun1)\ngrid.draw(lgd)"
  },
  {
    "objectID": "blogs/tools/09-tools.html#forest-plots",
    "href": "blogs/tools/09-tools.html#forest-plots",
    "title": "How to Plot Figures from My Previous Articles (R)",
    "section": "Forest Plots",
    "text": "Forest Plots\n\nRegular Forest Plots\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_fig2.knit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrm(list = ls())\nlibrary(forestplot)\nlibrary(dplyr)\n\ndata1 &lt;- read.csv(\"plot2a.csv\")\ndata1 &lt;- data1[data1$method == 'Inverse variance weighted',]\ndata1 &lt;- data1[c('exposure', 'outcome', 'or', 'or_lci95', 'or_uci95')]\ncolnames(data1) &lt;- c('labeltext', 'group', 'mean', 'lower', 'upper')\nhead(data1)\n##                labeltext                                 group      mean\n## 1   SARS-CoV-2 infection           1Cortical amyloid beta load 1.0470051\n## 2  Hospitalized COVID-19           1Cortical amyloid beta load 1.0169914\n## 3      Critical COVID-19           1Cortical amyloid beta load 1.0109526\n## 6   SARS-CoV-2 infection 2Alzheimers disease progression score 0.9886282\n## 9  Hospitalized COVID-19 2Alzheimers disease progression score 1.0002756\n## 12     Critical COVID-19 2Alzheimers disease progression score 0.9996817\n##        lower    upper\n## 1  0.6807174 1.610389\n## 2  0.9113855 1.134834\n## 3  0.9425631 1.084304\n## 6  0.8364508 1.168492\n## 9  0.9588436 1.043498\n## 12 0.9731772 1.026908\nfp1 &lt;- data1 |&gt; \n  group_by(group) |&gt;\n  forestplot(\n    clip = c(0.6, 1.6),\n    xticks = c(0.6, 0.8, 1.0, 1.2, 1.4, 1.6),\n    ci.vertices = TRUE,\n    ci.vertices.height = 0.05,\n    boxsize = .1,\n    xlab = \"OR (95% CI)\",\n    zero = 1,\n    title = \"                           HGI COVID-19\",\n    legend = c(\"Cortical amyloid beta load\", \"Alzheimers disease progression score\", \"Hippocampal volume\"),\n    #legend_args = fpLegend(pos = list(\"topright\")),\n    col = fpColors(box = c(\"darkblue\", \"darkred\", \"darkgreen\"),\n                   line = c(\"darkblue\", \"darkred\", \"darkgreen\")))\nplot(fp1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForest Plots with Groups\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_fig1.knit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrm(list = ls())\nlibrary(forestplot)\nlibrary(dplyr)\n\ndata1 &lt;- read.csv(\"plot1a.csv\")\ndata1 &lt;- data1[data1$method == 'Inverse variance weighted',]\ndata1 &lt;- data1[c('exposure',  'or', 'or_lci95', 'or_uci95', 'pval')]\ncolnames(data1) &lt;- c('labeltext',  'mean', 'lower', 'upper', 'pval')\nhead(data1)\n##                labeltext      mean     lower    upper      pval\n## 1   SARS-CoV-2 infection 0.9790143 0.8076468 1.186743 0.8289608\n## 4  Hospitalized COVID-19 0.9815525 0.9000865 1.070392 0.6736081\n## 7      Critical COVID-19 0.9755567 0.9239442 1.030052 0.3722147\n## 10    Critical COVID-19* 0.9776869 0.9246905 1.033721 0.4274140\nor_text1 &lt;- c()\np_text1 &lt;- c()\nfor (i in 1:dim(data1)[1]) {\n  or_temp &lt;- paste(as.character(round(data1$mean[i],3)), '(', \n                   as.character(round(data1$lower[i],3)),',',\n                   as.character(round(data1$upper[i],3)),')', sep = '')\n  or_text1 &lt;- c(or_text1, or_temp)\n  \n  p_text1 &lt;- c(p_text1, as.character(round(data1$pval[i],3)))\n}\n\nbase_data1 &lt;- tibble(mean=data1$mean,\n                     lower=data1$lower,\n                     upper=data1$upper, \n                     OR=or_text1,\n                     p=p_text1,\n                     exp_v=data1$labeltext)\n\n\nheader1 &lt;- tibble(OR=c(\"OR\"),\n                 exp_v=c(\"Exposure\"),\n                 p=c(\"P\"),\n                 summary=TRUE)\ndis_row1 &lt;- tibble(OR=c(NA),\n                  exp_v=c('Discovery'),\n                  p=c(NA),\n                  summary=TRUE)\n\noutput_df1 &lt;- bind_rows(header1, dis_row1, \n                       base_data1)\n\n\nfp1 &lt;- output_df1 |&gt;\n  forestplot(labeltext=c(exp_v, OR, p),\n             is.summary=summary,\n             shapes_gp = fpShapesGp(box = c(\"blue\") |&gt; lapply(function(x) gpar(fill = x, col = \"#555555\")),\n                                    default = gpar(vertices = TRUE)),\n             clip=c(0.97,1.03),\n             boxsize=.2,\n             #xlog=TRUE,\n             zero = 1,\n             xticks = c(0.91, 1.0, 1.09),\n             graph.pos = 3,\n             hrzl_lines = list(\"2\" = gpar(lty = 1)),\n             col = fpColors(box = \"royalblue\",\n                            line = \"darkblue\",\n                            summary = \"royalblue\"),\n             xlab = \"OR\",\n             vertices = TRUE)\nplot(fp1)"
  },
  {
    "objectID": "blogs/tools/07-tools.html",
    "href": "blogs/tools/07-tools.html",
    "title": "Basic Git Commands",
    "section": "",
    "text": "Source\nSource: bilibili"
  },
  {
    "objectID": "blogs/tools/07-tools.html#introduction",
    "href": "blogs/tools/07-tools.html#introduction",
    "title": "Basic Git Commands",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\n\n文件、状态、区域\n类别\n\n\n\n\n四个区域\n1. 工作区（working directory）：就是你在电脑里实际看到的目录； 2. 暂存区（stage/index）：暂存区也叫索引，用来临时存放未提交的内容，一般在.git目录下的index中； 3. 本地仓库（repository）：Git在本地的版本库，仓库信息存储在.git这个隐藏目录中； 4. 远程仓库（Remote Repository）：托管在远程服务器上的仓库。常用的有GitHub、GitLab、Gitee。\n\n\n文件状态\n1. 已修改（Modified）：修改了但是没有保存到暂存区的文件； 2. 已暂存（Staged）：修改后已经保存到暂存区的文件； 3. 已提交（Committed）：把暂存区的文件提交到本地仓库后的状态。\n\n\n状态\n1. main/master：默认主分支 2. Origin：默认远程仓库 3. HEAD：指向当前分支的指针 4. HEAD^：上一个版本 5. HEAD~：上四个版本\n\n\n特殊文件\n1. .git：Git仓库的元数据和对象数据库 2. .gitignore：忽略文件，不需要提交到仓库的文件 3. .gitattributes：指向当前分支的指针 4. .gitkeep：使空目录被提交到仓库 5. .gitmodules：记录子模块的信息 6. .gitconfig：记录仓库的配置信息\n\n\nGitFlow\n1. 主分支（master/main）：代表了项目的稳定版本，每个提交到主分支的代码都应该是经过测试和审核的；2. 开发分支（develop）：用于日常开发。所有的功能分支、发布分支和修补分支都应该从开发分支派生出来；3. 功能分支（feature）：用于开发单独的功能或者特性。每个功能分支都应该从开发分支派生，并在开发完成后合并回开发分支；4. 发布分支（release）：用于准备项目发布。发布分支应该从开发分支派生，并在准备好发布版本后合并回主分支和开发分支；5. 热修复分支（hotfix）：用于修复主分支上的紧急问题。热修复分支应该从主分支派生，并在修复完成后，合并回主分支和开发分支。"
  },
  {
    "objectID": "blogs/tools/07-tools.html#list-of-basic-git-commands",
    "href": "blogs/tools/07-tools.html#list-of-basic-git-commands",
    "title": "Basic Git Commands",
    "section": "List of Basic Git Commands",
    "text": "List of Basic Git Commands\n\n\n\n\n\n\n\n\n类别\nCommand\nUsage\n\n\n\n\n初始化设置\ngit config --global user.name \"Your Name\"\n配置用户名\n\n\n\ngit config --global user.email \"mail@example.com\"\n配置邮箱\n\n\n\ngit config --global credential store\n存储配置\n\n\n创建仓库\ngit init &lt;project-name&gt;\n创建一个新的本地仓库（省略project-name则在当前目录创建）\n\n\n\ngit clone &lt;url&gt;\n克隆一个远程仓库\n\n\n分支\ngit branch\n查看所有本地的分支，当前分支前面会有一个星号*，-r查看远程分支，-a查看所有分支。\n\n\n\ngit branch &lt;branch-name&gt;\n创建一个新的分支\n\n\n\ngit checkout -b &lt;branch-name&gt;\n切换到指定分支，并更新工作区\n\n\n\ngit branch -d &lt;branch-name&gt;\n删除一个已经合并的分支\n\n\n\ngit checkout -D &lt;branch-name&gt;\n删除一个分支，不管是否合并\n\n\n\ngit tag &lt;tag-name&gt;\n给当前的提交打上标签，通常用于版本发布\n\n\n\ngit merge --no-ff -m message &lt;branch-name&gt;\n合并分支，–no-ff参数表示禁用FastForward模式，合并后的历史有分支，能看出曾经做过的合并，而-ff参数表示使用FastForward模式，合并后的历史会变成一条直线\n\n\n\ngit squash &lt;branch-name&gt;\n合并&挤压（squash）所有提交到一个提交\n\n\n\ngit switch &lt;branch-name&gt;\nGit switch is a command introduced in Git version 2.23 as a replacement for branch switching, providing better safety checks and usability.\n\n\n\ngit checkout &lt;branch-name&gt;\ngit checkout is a versatile command used not only for branch switching but also for file and commit operations.\n\n\n\ngit rebase &lt;main&gt;\nrebase操作可以把本地未push的分叉提交历史整理成直线，看起来更加直观。但是，如果多人协作时，不要对已经推送到远程的分支执行rebase操作。rebase不会产生新的提交，而是把当前分支的每一个提交都”复制”到目标分支上，然后再把当前分支指向目标分支，而merge会产生一个新的提交，这个提交有两个分支的所有修改。\n\n\n撤销和恢复\ngit mv &lt;file&gt; &lt;new-file&gt;\n移动一个文件到新的位置\n\n\n\ngit rm &lt;file&gt;\n从工作区和暂存区删除一个文件，并且将这次删除放入暂存区\n\n\n\ngit rm --cached\n从索引/暂存区中删除文件，但是本地工作区文件还在，只是不希望这个文件被版本控制。\n\n\n\ngit checkout &lt;file&gt; &lt;commit-id&gt;\n恢复一个文件到之前的版本\n\n\n\ngit revert &lt;commit-id&gt;\n创建一个新的提交，用来撤销指定的提交，后者的所有变化将被前者抵消，并且应用到当前分支\n\n\n\ngit reset --mixed &lt;commit-id&gt;\n重置当前分支的HEAD为之前的某个提交，并且删除所有之后的提交。 –hard参数表示重置工作区的暂存区  –soft参数表示重置暂存区  –mixed参数表示重置工作区\n\n\n\ngit restore --staged &lt;file&gt;\n撤销暂存区的文件，重新放回工作区（git add的反向操作)\n\n\n查看状态或差异\ngit status\n查看仓库状态，列出还未提交的新的或修改的文件\n\n\n\ngit log --online\n查看提交历史，–online表示简介模式\n\n\n\ngit diff\n查看未暂存的文件更新了哪些部分\n\n\n\ngit diff &lt;commit-id&gt; &lt;commit-id&gt;\n查看两个提交之间的差异\n\n\nStash\ngit status save \"message\"\nStash操作可以把当前工作现场”储存”起来，等以后恢复现场后继续工作。  -u参数表示把所有未跟踪的文件也一并存储； -a参数表示把所有未跟踪的文件和忽略的文件也一并存储； save参数表示存储的信息，可以不写\n\n\n\ngit stash list\n查看所有的stash\n\n\n\ngit stash pop\n恢复最近一次stash\n\n\n\ngit stash pop stash@{2}\n恢复指定的stash，stash@{2}表示第三个stash，stash@{0}表示最近的stash\n\n\n\ngit stash apply\n重新接受最近一次stash\n\n\n\ngit stash drop stash@{2}\npop和apply的区别是，pop会把stash内容删除，而apply不会。可以使用git stash drop来删除stash\n\n\n\ngit stash clear\n删除所有stash\n\n\n远程仓库\ngit remote add &lt;remote-name&gt; &lt;remote-url&gt;\n添加远程仓库\n\n\n\ngit remote -v\n查看远程仓库\n\n\n\ngit remote rm &lt;remote-name&gt;\n删除远程仓库\n\n\n\ngit remote rename &lt;old-name&gt; &lt;new-name&gt;\n重命名远程仓库\n\n\n\ngit pull &lt;remote-name&gt; &lt;branch-name&gt;\n从远程仓库拉取代码。默认拉取远程仓库名origin的master或者main分支\n\n\n\ngit pull --rebase\n将本地改动的代码rebase到远程仓库的最新代码上（为了有一个干净、线性的提交历史）\n\n\n\ngit push &lt;remote-name&gt; &lt;branch-name&gt;\n推送代码到远程仓库（然后再发起pull request）\n\n\n\ngit fetch &lt;remote-name&gt;\n获取所有远程分支\n\n\n\ngit branch -r\n查看远程分支\n\n\n\ngit fetch &lt;remote-name&gt; &lt;branch-name&gt;\nFetch某一个特定的远程分支"
  },
  {
    "objectID": "blogs/tools/07-tools.html#detailed-examples-on-github",
    "href": "blogs/tools/07-tools.html#detailed-examples-on-github",
    "title": "Basic Git Commands",
    "section": "Detailed Examples on GitHub",
    "text": "Detailed Examples on GitHub\n\n\n\n\n\n\n\nExamples\nSteps\n\n\n\n\n将GitHub代码仓库下载本地\n1. 首先你在GitHub上创建一个仓库：然后给这个仓库添加你的ssh密匙，这一步大家可以参考网上教程，可以去百度搜索：“如何给GitHub创建仓库和添加ssh” 2. 进入仓库，选择code下的ssh复制按钮。 3. 在本地新建一个空白文件夹用于下载GitHub代码仓库（ps:必须保证是空的文件夹） 4. 在该文件目录下右键选择”Git bash here” 5. 输入pwd查看当前位置是否为你所建立的空白文件夹位置，却无误后输入：git clone 加上你复制的SSH密匙。比如:git clone git@github.com:KDDing/GLP1R_Private.git 6. 完成从GitHub上下载代码仓库\n\n\n将在GitHub上更改的内容同步到本地\n1. 在给GitHub上修改完后记得保存 (commit changes on GitHub web) 2. 进入你建立的文件夹，右键git bash here 3. 输入pwd确认位置后，输入git pull origin 4. 查看本地是否更新\n\n\n将本地更新后的代码上传到GitHub\n1. 更改本地文件 2. 在文件夹下右键git bash here,输入pwd确认位置 3. 输入git add . 4. 输入git commit -m 'update' 5. 输入git push 6. 打开GitHub查看是否成功\n\n\n关联本地仓库和远程仓库\n1. git remote add origin git\\@github.com:geekhall-laoyang/first-repo.git 2. git branch -M main 3. git push -u origin main\n\n\nRelieve file folder with Git repository\nfind . -name \".git\" | xargs rm -Rf\n\n\nIgnoring Files\nOften, you’ll have a class of files that you don’t want Git to automatically add or even show you as being untracked. These are generally automatically generated files such as log files or files produced by your build system. In such cases, you can create a file listing patterns to match them named .gitignore. Here is an example .gitignore file:  1. cat .gitignore 2. *.[oa] 3. *~  The first line tells Git to ignore any files ending in “.o” or “.a” — object and archive files that may be the product of building your code. The second line tells Git to ignore all files whose names end with a tilde (~), which is used by many text editors such as Emacs to mark temporary files. You may also include a log, tmp, or pid directory; automatically generated documentation; and so on. Setting up a .gitignore file for your new repository before you get going is generally a good idea so you don’t accidentally commit files that you really don’t want in your Git repository."
  },
  {
    "objectID": "blogs/tools/05-tools.html",
    "href": "blogs/tools/05-tools.html",
    "title": "Build Your Own VPN in AWS and OpenVPN",
    "section": "",
    "text": "登录AWS控制台并选择相应服务器区域\n\n\n\n转至EC2云中虚拟服务器\n\n\n\n安装OpenVPN Access Server（1CPU，1GB免费）\n\n\n\n创建密钥对\n\n\n\n密钥放在相应文件夹\n\n\n\n创建弹性IP地址并关联\n\n\n\n利用终端连接EC2\n\n\n\n利用PowerShell安装初始化OpenVPN Client\n\n\n\n这个地方不是默认的的no（密码敲入自己容易记住的密码）\n\n\n\n在网页中登录OpenVPN Admin和客服端\n\n\n\n安装OpenVPN终端\n\n\n\n删除原有profile并下载导入自己的profile\n\n\nSource: Youtube"
  },
  {
    "objectID": "blogs/tools/03-tools.html",
    "href": "blogs/tools/03-tools.html",
    "title": "Change Jupyter Satrtup Folder",
    "section": "",
    "text": "Launch Anaconda Prompt.\nrun jupyter notebook --generate-config to generate the configuration file.\n\nThis writes a file to C:\\Users\\dpjhn\\.jupyter\\jupyter_notebook_config.py (dpjhn is my username).\nBrowse to the file location and open it in an Editor (such as: PyCharm).\nSearch for the following line in the file: c.NotebookApp.notebook_dir.\nReplace by D:\\\\Jupyter_note, and remove the # at the beginning the line to allow the line to execute\n\nBrowse to the location of Jupyter Notebook, (such as: C:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\Anaconda3 (64-bit)\\Jupyter Notebook (Anaconda3))\n(the file of Jupyter Notebook (Anaconda3)) Properties -&gt; Shortcut -&gt; Target, remove parameters \"%USERPROFILE%/\".\nRestart the Jupyter Lab (or Notebook).\n\nSource: Stack Overflow"
  },
  {
    "objectID": "blogs/tools/01-tools.html",
    "href": "blogs/tools/01-tools.html",
    "title": "How To Setup Jupyter Notebook In Conda Environment And Install Kernel",
    "section": "",
    "text": "I list the necessary commands here."
  },
  {
    "objectID": "blogs/tools/01-tools.html#how-to-setup-jupyter-notebook-in-conda-environment-and-install-kernel",
    "href": "blogs/tools/01-tools.html#how-to-setup-jupyter-notebook-in-conda-environment-and-install-kernel",
    "title": "How To Setup Jupyter Notebook In Conda Environment And Install Kernel",
    "section": "How To Setup Jupyter Notebook In Conda Environment And Install Kernel",
    "text": "How To Setup Jupyter Notebook In Conda Environment And Install Kernel\nAssuming your conda-env is named ml, it is as simple as:\nconda activate ml\nconda install ipykernel\nipython kernel install --user --name=&lt;any_name_for_kernel&gt;\nconda deactivate\nconda install ipykernel installs all dependencies needed to use jupyter.\nipython kernel install --user --name=any_name_for_kernel installs the kernel for this environment. I usually use the same kernel name as the environment name here (i.e. ml in this example)."
  },
  {
    "objectID": "blogs/tools/01-tools.html#how-to-removeuninstall-a-kernel-again",
    "href": "blogs/tools/01-tools.html#how-to-removeuninstall-a-kernel-again",
    "title": "How To Setup Jupyter Notebook In Conda Environment And Install Kernel",
    "section": "How To Remove/Uninstall a Kernel Again¶",
    "text": "How To Remove/Uninstall a Kernel Again¶\nRun jupyter kernelspec list to get the paths of all your kernels.\njupyter kernelspec list\nThen simply uninstall your unwanted-kernel:\njupyter kernelspec uninstall unwanted-kernel\nSource"
  },
  {
    "objectID": "blogs/statistics/05-stats.html",
    "href": "blogs/statistics/05-stats.html",
    "title": "Understanding Instrumental Variables",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "blogs/statistics/05-stats.html#introduction",
    "href": "blogs/statistics/05-stats.html#introduction",
    "title": "Understanding Instrumental Variables",
    "section": "Introduction",
    "text": "Introduction\nInstrumental variables (IVs) are used to control for confounding and measurement error in observational studies. They allow for the possibility of making causal inferences with observational data. Like propensity scores, IVs can adjust for both observed and unobserved confounding effects. Other methods of adjusting for confounding effects, which include stratification, matching and multiple regression methods, can only adjust for observed confounders. IVs have primarily been used in economics research, but have recently begun to appear in epidemiological studies.\nObservational studies are often implemented as a substitute for or complement to clinical trials, although clinical trials are the gold standard for making causal inference. The main concern with using observational data to make causal inferences is that an individual may be more likely to receive a treatment because that individual has one or more co-morbid conditions. The outcome may be influenced by the fact that some individuals received the treatment because of their personal or health characteristics.\nConsider the linear regression model:\n\\[\ny_i = b_0 + b_1X_{1i} + b_2X_{2i} + ... + b_kX_{ki} + u_i\n\\]\nwhere \\(y_i\\) is the outcome for the ith individual; \\(X_{mi}\\) is the m-th explanatory variable (m=1,…,k) for the i-th individual; \\(b_m\\) is the parameter associated with the m-th explanatory variable; \\(u_i\\) is the random error term for the i-th individual.\nLet Z denote a randomization assignment indicator variable in this regression model, such that Z=1 when a treatment is received and Z=0 when the control or placebo is received, and let \\(X_1\\) be the treatment. Z is referred to as the instrumental variable because it satisfies the following conditions: (i) Z has a causal effect on X; (ii) Z affects the outcome variable Y only through X (Z does not have a direct influence on Y which is referred to as the exclusion restriction); (iii) There is no confounding for the effect of Z on Y.\n\nThere are two main criteria for defining an IV: (i) It causes variation in the treatment variable; (ii) It does not have a direct effect on the outcome variable, only indirectly through the treatment variable.\nA reliable implementation of an IV must satisfy these two criteria and utilize a sufficient sample size to allow for reasonable estimation of the treatment effect. If the second assumption is not satisfied, implying that the IV is associated with the outcome, then estimation of the IV effect may be biased. If the first assumption is not satisfied, implying that the IV does not affect the treatment variable then the random error will tend to have the same effect as the treatment. When selecting an IV, one must ensure that it only affects whether or not the treatment is received and is not associated with the outcome variable.\nAlthough IVs can control for confounding and measurement error in observational studies, they have some limitations. We must be careful when dealing with many confounders and also if the correlation between the IV and the exposure variables is small. Both weak instruments and confounders produce large standard error which results in imprecise and biased results. Even when the two key assumptions are satisfied and the sample size is large, IVs cannot be used as a subsitute for the use of clinical trials to make causal inference, although they are often useful in answering questions that an observational study can not. In general, instrumental variables are most suitable for studies in which there are only moderate to small confounding effects. They are least useful when there are strong confounding effects."
  },
  {
    "objectID": "blogs/statistics/05-stats.html#instrumental-variables-a-brief-annotated-bibliography",
    "href": "blogs/statistics/05-stats.html#instrumental-variables-a-brief-annotated-bibliography",
    "title": "Understanding Instrumental Variables",
    "section": "Instrumental Variables: A Brief Annotated Bibliography",
    "text": "Instrumental Variables: A Brief Annotated Bibliography\n\nEconomics nobelprize.org\nMost applied science is concerned with uncovering causal relationships. In many fields, randomized controlled trials (RCTs) are considered the gold standard for achieving this. The systematic use of RCTs to study causal relationships — assessing the efficacy of a medical treatment for example — has resulted in tremendous welfare gains in society. However, due to financial, ethical, or practical constraints, many important questions — particularly in the social sciences — cannot be studied using a controlled randomized experiment. For example, what is the impact of school closures on student learning and the spread of the COVID-19 virus? What is the impact of low-skilled immigration on employment and wages? How do institutions affect economic development? How does the imposition of a minimum wage affect employment? In answering these types of questions, researchers must rely on observational data, i.e., data generated without controlled experimental variation. But with observational data, a fundamental identification problem arises: the underlying cause of any correlation remains unclear. If we observe that minimum wages and unemployment correlate, is this because a minimum wage causes unemployment? Or because unemployment and lower wage growth at the bottom of the wage distribution leads to the introduction of a minimum wage? Or because of a myriad of other factors that affect both unemployment and the decision to introduce a minimum wage? Moreover, in many settings, randomized variation by itself is not sufficient for identification of an average treatment effect.\nThis year’s (2021) Prize in Economic Sciences rewards three scholars: David Card of the University of California, Berkeley, Joshua Angrist of Massachusetts Institute of Technology, and Guido Imbens of Stanford University. The Laureates’ contributions are separate but complementary. Starting with a series of paper from the early 1990s, David Card began to analyze a number of core questions in labor economics using “natural experiments”, i.e., a study design in which the units of analysis are exposed to as good as random variation caused by nature, institutions, or policy changes. These initial studies — on the minimum wage, on the impact of immigration, and on education policy — challenged conventional wisdom, and were also the starting point of an iterative process of replications, new empirical studies, and theoretical work, with Card remaining a core contributor. Thanks to this work, we have gained a much deeper understanding of how labor markets operate.\nIn the mid-1990s, Joshua Angrist and Guido Imbens made fundamental contributions to the challenge of estimating an average treatment effect. In particular, they analyzed the realistic scenario when individuals are affected differently by the treatment and choose whether to comply with the assignment generated by the natural experiment. Angrist and Imbens showed that even in this general setting it is possible to estimate a well-defined treatment effect — the local average treatment effect (LATE) — under a set of minimal (and in many cases empirically plausible) conditions. In deriving their key results, they merged the instrumental variables (IV) framework, common in economics, with the potential-outcomes framework for causal inference, common in statistics. Within this framework, they clarified the core identifying assumptions in a causal design and provided a transparent way of investigating the sensitivity to violations of these assumptions. The combined contribution of the Laureates, however, is larger than the sum of the individual parts. Card’s studies from the early 1990s showcased the power of exploiting natural experiments to uncover causal effects in important domains. This early work thus played a crucial role in shifting the focus in empirical research using observational data towards relying on quasiexperimental variation to establish causal effects. The framework developed by Angrist and Imbens, in turn, significantly altered how researchers approach empirical questions using data generated from either natural experiments or randomized experiments with incomplete compliance to the assigned treatment. At the core, the LATE interpretation clarifies what can and cannot be learned from such experiments. Taken together, therefore, the Laureates’ contributions have played a central role in establishing the so-called design-based approach in economics. This approach – aimed at emulating a randomized experiment to answer a causal question using observational data – has transformed applied work and improved researchers’ ability to answer causal questions of great importance for economic and social policy using observational data.\n\n\nEpidemiology Epidemiology\nIn medical research, randomized, controlled trials (RCTs) remain the gold standard in assessing the effect of one variable of interest, often a specified treatment. Nevertheless, observational studies are often used in estimating such an effect.1 In epidemiologic as well as sociologic and economic research, observational studies are the standard for exploring causal relationships between an exposure and an outcome variable. The main problem of estimating the effect in such studies is the potential bias resulting from confounding between the variable of interest and alternative explanations for the outcome (confounders). Traditionally, standard methods such as stratification, matching, and multiple regression techniques have been used to deal with confounding. In the epidemiologic literature, some other methods have been proposed 2,3 of which the method of propensity scores is best known.4 In most of these methods, adjustment can be made only for observed confounders.\nA method that has the potential to adjust for all confounders, whether observed or not, is the method of instrumental variables (IV). This method is well known in economics and econometrics as the estimation of simultaneous regression equations 5 and is also referred to as structural equations and two-stage least squares. This method has a long tradition in economic literature, but has entered more recently into the medical research literature with increased focus on the validity of the instruments. Introductory texts on instrumental variables can be found in Greenland 6 and Zohoori and Savitz. 7\nOne of the earliest applications of IV in the medical field is probably the research of Permutt and Hebel, 8 who estimated the effect of smoking of pregnant women on their child’s birth weight, using an encouragement to stop smoking as the instrumental variable. More recent examples can be found in Beck et al, 9 Brooks et al, 10 Earle et al, 11 Hadley et al, 12 Leigh and Schembri, 13 McClellan, 14 and McIntosh. 15 However, it has been argued that the application of this method is limited because of its strong assumptions, making it difficult in practice to find a suitable instrumental variable. 16"
  },
  {
    "objectID": "blogs/statistics/03-stats.html",
    "href": "blogs/statistics/03-stats.html",
    "title": "Selection Bias",
    "section": "",
    "text": "Source: Catalogue of Bias\noccurs when individuals or groups in a study differ systematically from the population of interest leading to a systematic error in an association or outcome."
  },
  {
    "objectID": "blogs/statistics/03-stats.html#background",
    "href": "blogs/statistics/03-stats.html#background",
    "title": "Selection Bias",
    "section": "Background",
    "text": "Background\nParticipants in research may differ systematically from the population of interest. For example, participants included in an influenza vaccine trial may be healthy young adults, whereas those who are most likely to receive the intervention in practice may be elderly and have many comorbidities, and are therefore not representative. Similarly, in observational studies, conclusions from the research population may not apply to real-world people, as the observed effect may be exaggerated or it is not possible to assume an effect in those not included in the study.\nSelection bias can arise in studies because groups of participants may differ in ways other than the interventions or exposures under investigation. When this is the case, the results of the study are biased by confounding."
  },
  {
    "objectID": "blogs/statistics/03-stats.html#example",
    "href": "blogs/statistics/03-stats.html#example",
    "title": "Selection Bias",
    "section": "Example",
    "text": "Example\nA study of the prevalence of Parkinson’s disease (PD) completed a door to door survey of an entire US county. They used a two-stage screening technique, first administering a comprehensive questionnaire and then referring those subjects with signs or symptoms suggestive of PD for a neurological evaluation. Over 97% of the households in the county participated. Some 15% of those screening positive in the first screen refused follow-up. Extensive efforts provided ‘valuable information on almost all the refusals which was reviewed by a neurologist to establish a diagnosis’. The authors presented convincing evidence that they had succeeded in obtaining a complete enumeration of PD cases in the county.\nOf the approximately 24,000 residents on prevalence day, 1 January 1978, PD was diagnosed in 31 participants. Thirteen of those 31 had never been seen for medical care. In this survey, if another approach to the ascertainment of cases had used only the medical care system, all of those who had not received care (over 40%) would not have been identified. Furthermore, there would have been no definitive way of characterizing the bias introduced if only those identified via health records were used.\nAnother example is the effect of HRT on coronary heart disease (CHD) in women. Several studies showed that HRT reduced coronary heart disease (CHD), but subsequent RCTs showed that HRT might increase the risk of CHD disease. The Women in the observational studies on HRT were more health conscious, more physically active, and had higher socioeconomic status than those not on HRT. This self-selection of women (selection bias) led to confounding and a “healthy-user bias”.\nIn a double-blind placebo-controlled study of the use of phenobarbital for prevention of recurrent febrile seizures, adherence was not as good as hoped. The Kaplan-Meier curves for remaining free from seizures were not statistically significantly different from each other, contrary to expectation. The authors used several definitions of adherence and reanalysed the results ‘as treated’. The results based on one definition of adherence showed that adherent subjects in both the phenobarbital and the placebo groups had a higher risk of recurrence than those who were non-adherent. The results were thus conflicting and demonstrated selection bias due to attrition.\nProspective cohort studies of dietary and lifestyle factors exhibit a “healthy participant effect”, reporting lower mortality rates among participants than among the general population. This suggests that people who are interested in healthy lifestyles, and therefore have more healthy behaviours, such as low smoking rates, are more likely to sign up to take part in a prospective study than those with less healthy lifestyles. This can also be considered a sampling bias\nA study of cigarette smoking and dementia found potential selection bias in the elderly. Selection bias due to censoring by death was one explanation for the lower relative rate of dementia in smokers with increasing age."
  },
  {
    "objectID": "blogs/statistics/03-stats.html#impact",
    "href": "blogs/statistics/03-stats.html#impact",
    "title": "Selection Bias",
    "section": "Impact",
    "text": "Impact\nSelection bias can have varying effects, and the magnitude of its impact and the direction of the effect is often hard to determine. (Odgaard-Jensen J et al.)\nAs examples, a meta-epidemiological study of the Impact of Selection Bias on Treatment Effect Size Estimates in Randomized Trials of Oral Health Interventions found significantly larger treatment effect estimates in trials that had inadequate/unknown sequence generation (difference in ES = 0.13; 95% CI: 0.01 to 0.25). A further study to determine survival in preterm infant cohort studies found that the presence of selection bias overestimated survival by as much as 100%."
  },
  {
    "objectID": "blogs/statistics/03-stats.html#preventive-steps",
    "href": "blogs/statistics/03-stats.html#preventive-steps",
    "title": "Selection Bias",
    "section": "Preventive steps",
    "text": "Preventive steps\nTo assess the probable degree of selection bias, authors should include the following information at different stages of the trial or study:\n\nNumbers of participants screened as well as randomised/included.\nHow intervention/exposure groups compared at baseline.\nTo what extent potential participants were re-screened.\nExactly what procedures were put in place to prevent prediction of future allocations and knowledge of previous allocations.\nWhat the restrictions were on randomisation, e.g. block sizes.\nAny evidence of unblinding.\nHow missing data from participants lost to follow-up were handled.\n\nRandomisation of participants in intervention studies aims to provide the fairest method of comparing the effect of an intervention with a control, and preventing selection biases is part of this aim. However, it may not be perfectly achieved. Berger and colleagues have made clear arguments for the crucial role of adequate allocation concealment and randomisation procedures to prevent selection biases.\nBecause anything that happens after randomisation can affect the chance that a study participant has the outcome of interest, it is essential that all patients (even those who fail to take their medicine or accidentally or intentionally receive the wrong treatment) are analysed in the groups to which they were allocated. The intention-to-treat analysis includes data from all the participants randomly assigned to the treatment comparison groups, whether or not they received the treatment to which they were assigned, even if they never started the treatment, or switched to a different one during the study. This prevents bias caused by disruption of the baseline equivalence established by random allocation.\nIn Observational studies selection bias is difficult to address with analytical methods, but methods for dealing with missing data are available, including last observation (or baseline value) carried forward, mixed models, imputation, and sensitivity analysis using ‘worst case’ scenarios (assuming that those with no information all got worse) and ‘best case’ scenarios (assuming that all got better). Analysing data only from participants remaining in the study is called complete case analysis.\nCertain external measures can sometimes be used to calibrate the data from a study, an example being standardised mortality rates. Moreover, inverse probability weighting can be used under certain assumptions.\nTo improve generalisability of study findings the selection of the population should be broad and reported in the recruitment/inclusion criteria."
  },
  {
    "objectID": "blogs/statistics/01-stats.html",
    "href": "blogs/statistics/01-stats.html",
    "title": "What Are Degrees of Freedom in Statistics?",
    "section": "",
    "text": "Source\nAbout a year ago, a reader asked if I could try to explain degrees of freedom in statistics. Since then, I’ve been circling around that request very cautiously, like it’s some kind of wild beast that I’m not sure I can safely wrestle to the ground.\nDegrees of freedom aren’t easy to explain. They come up in many different contexts in statistics—some advanced and complicated. In mathematics, they’re technically defined as the dimension of the domain of a random vector.\nBut we won’t get into that. Because degrees of freedom are generally not something you need to understand to perform a statistical analysis—unless you’re a research statistician, or someone studying statistical theory.\nAnd yet, enquiring minds want to know. So for the adventurous and the curious, here are some examples that provide a basic gist of their meaning in statistics."
  },
  {
    "objectID": "blogs/statistics/01-stats.html#the-freedom-to-vary",
    "href": "blogs/statistics/01-stats.html#the-freedom-to-vary",
    "title": "What Are Degrees of Freedom in Statistics?",
    "section": "THE FREEDOM TO VARY",
    "text": "THE FREEDOM TO VARY\nFirst, forget about statistics. Imagine you’re a fun-loving person who loves to wear hats. You couldn’t care less what a degree of freedom is. You believe that variety is the spice of life.\nUnfortunately, you have constraints. You have only 7 hats. Yet you want to wear a different hat every day of the week.\nOn the first day, you can wear any of the 7 hats. On the second day, you can choose from the 6 remaining hats, on day 3 you can choose from 5 hats, and so on.\nWhen day 6 rolls around, you still have a choice between 2 hats that you haven’t worn yet that week. But after you choose your hat for day 6, you have no choice for the hat that you wear on Day 7. You must wear the one remaining hat. You had 7-1 = 6 days of “hat” freedom—in which the hat you wore could vary!\nThat’s kind of the idea behind degrees of freedom in statistics. Degrees of freedom are often broadly defined as the number of “observations” (pieces of information) in the data that are free to vary when estimating statistical parameters."
  },
  {
    "objectID": "blogs/statistics/01-stats.html#degrees-of-freedom-1-sample-t-test",
    "href": "blogs/statistics/01-stats.html#degrees-of-freedom-1-sample-t-test",
    "title": "What Are Degrees of Freedom in Statistics?",
    "section": "DEGREES OF FREEDOM: 1-SAMPLE T TEST",
    "text": "DEGREES OF FREEDOM: 1-SAMPLE T TEST\nNow imagine you’re not into hats. You’re into data analysis.\nYou have a data set with 10 values. If you’re not estimating anything, each value can take on any number, right? Each value is completely free to vary.\nBut suppose you want to test the population mean with a sample of 10 values, using a 1-sample t test. You now have a constraint—the estimation of the mean. What is that constraint, exactly? By definition of the mean, the following relationship must hold: The sum of all values in the data must equal n x mean, where n is the number of values in the data set.\nSo if a data set has 10 values, the sum of the 10 values must equal the mean x 10. If the mean of the 10 values is 3.5 (you could pick any number), this constraint requires that the sum of the 10 values must equal 10 x 3.5 = 35.\nWith that constraint, the first value in the data set is free to vary. Whatever value it is, it’s still possible for the sum of all 10 numbers to have a value of 35. The second value is also free to vary, because whatever value you choose, it still allows for the possibility that the sum of all the values is 35.\nIn fact, the first 9 values could be anything, including these two examples:\n34, -8.3, -37, -92, -1, 0, 1, -22, 99\n0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\nBut to have all 10 values sum to 35, and have a mean of 3.5, the 10th value cannot vary. It must be a specific number:\n34, -8.3, -37, -92, -1, 0, 1, -22, 99  -----&gt; 10TH value must be 61.3\n0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 ----&gt; 10TH value must be 30.5\nTherefore, you have 10 - 1 = 9 degrees of freedom. It doesn’t matter what sample size you use, or what mean value you use—the last value in the sample is not free to vary. You end up with n - 1 degrees of freedom, where n is the sample size.\nAnother way to say this is that the number of degrees of freedom equals the number of “observations” minus the number of required relations among the observations (e.g., the number of parameter estimates). For a 1-sample t-test, one degree of freedom is spent estimating the mean, and the remaining n - 1 degrees of freedom estimate variability.\nThe degrees for freedom then define the specific t-distribution that’s used to calculate the p-values and t-values for the t-test.\nNotice that for small sample sizes (n), which correspond with smaller degrees of freedom (n - 1 for the 1-sample t test), the t-distribution has fatter tails. This is because the t distribution was specially designed to provide more conservative test results when analyzing small samples (such as in the brewing industry). As the sample size (n) increases, the number of degrees of freedom increases, and the t-distribution approaches a normal distribution."
  },
  {
    "objectID": "blogs/statistics/01-stats.html#degrees-of-freedom-chi-square-test-of-independence",
    "href": "blogs/statistics/01-stats.html#degrees-of-freedom-chi-square-test-of-independence",
    "title": "What Are Degrees of Freedom in Statistics?",
    "section": "DEGREES OF FREEDOM: CHI-SQUARE TEST OF INDEPENDENCE",
    "text": "DEGREES OF FREEDOM: CHI-SQUARE TEST OF INDEPENDENCE\nLet’s look at another context. A chi-square test of independence is used to determine whether two categorical variables are dependent. For this test, the degrees of freedom are the number of cells in the two-way table of the categorical variables that can vary, given the constraints of the row and column marginal totals.So each “observation” in this case is a frequency in a cell.\nConsider the simplest example: a 2 x 2 table, with two categories and two levels for each category:\n\n\n\n\nCategory A\nCategory A\nTotal\n\n\n\n\nCategory B\n?\n\n6\n\n\nCategory B\n\n\n15\n\n\nTotal\n10\n11\n21\n\n\n\nIt doesn’t matter what values you use for the row and column marginal totals. Once those values are set, there’s only one cell value that can vary (here, shown with the question mark—but it could be any one of the four cells). Once you enter a number for one cell, the numbers for all the other cells are predetermined by the row and column totals. They’re not free to vary. So the chi-square test for independence has only 1 degree of freedom for a 2 x 2 table.\nSimilarly, a 3 x 2 table has 2 degrees of freedom, because only two of the cells can vary for a given set of marginal totals.\n\n\n\n\nCategory A\nCategory A\nCategory A\nTotal\n\n\n\n\nCategory B\n?\n?\n\n15\n\n\nCategory B\n\n\n\n15\n\n\nTotal\n10\n11\n9\n30\n\n\n\nIf you experimented with different sized tables, eventually you’d find a general pattern. For a table with r rows and c columns, the number of cells that can vary is (r-1)(c-1). And that’s the formula for the degrees for freedom for the chi-square test of independence!\nThe degrees of freedom then define the chi-square distribution used to evaluate independence for the test.\nThe chi-square distribution is positively skewed. As the degrees of freedom increases, it approaches the normal curve."
  },
  {
    "objectID": "blogs/statistics/01-stats.html#degrees-of-freedom-regression",
    "href": "blogs/statistics/01-stats.html#degrees-of-freedom-regression",
    "title": "What Are Degrees of Freedom in Statistics?",
    "section": "DEGREES OF FREEDOM: REGRESSION",
    "text": "DEGREES OF FREEDOM: REGRESSION\nDegrees of freedom is more involved in the context of regression. Rather than risk losing the one remaining reader still reading this post (hi, Mom!), I’ll cut to the chase.\nRecall that degrees of freedom generally equals the number of observations (or pieces of information) minus the number of parameters estimated. When you perform regression, a parameter is estimated for every term in the model, and and each one consumes a degree of freedom. Therefore, including excessive terms in a multiple regression model reduces the degrees of freedom available to estimate the parameters’ variability. In fact, if the amount of data isn’t sufficient for the number of terms in your model, there may not even be enough degrees of freedom (DF) for the error term and no p-value or F-values can be calculated at all. You’ll get output something like this:\nIf this happens, you either need to collect more data (to increase the degrees of freedom) or drop terms from your model (to reduce the number of degrees of freedom required). So degrees of freedom does have real, tangible effects on your data analysis, despite existing in the netherworld of the domain of a random vector."
  },
  {
    "objectID": "blogs/statistics/01-stats.html#follow-up",
    "href": "blogs/statistics/01-stats.html#follow-up",
    "title": "What Are Degrees of Freedom in Statistics?",
    "section": "FOLLOW-UP",
    "text": "FOLLOW-UP\nThis post provides a basic, informal introduction to degrees of freedom in statistics. If you want to further your conceptual understanding of degrees of freedom, check out this classic paper in the Journal of Educational Psychology by Dr. Helen Walker, an associate professor of education at Columbia who was the first female president of the American Statistical Association. Another good general reference is by Pandy, S., and Bright, C. L., Social Work Research Vol 32, number 2, June 2008, available here."
  },
  {
    "objectID": "blogs/bioinformatics/09-bio.html",
    "href": "blogs/bioinformatics/09-bio.html",
    "title": "RNA Sequencing",
    "section": "",
    "text": "Source: illumina\nArticle: From RNA-seq reads to differential expression results"
  },
  {
    "objectID": "blogs/bioinformatics/09-bio.html#processing-high-throughput-rna-sequencing-data-and-detecting-differential-expression",
    "href": "blogs/bioinformatics/09-bio.html#processing-high-throughput-rna-sequencing-data-and-detecting-differential-expression",
    "title": "RNA Sequencing",
    "section": "Processing high-throughput RNA sequencing data and detecting differential expression",
    "text": "Processing high-throughput RNA sequencing data and detecting differential expression\nDNA may get most of the public’s attention, but it is gene expression and regulation that orchestrate the dynamics of cell function and physiology. The majority of variants identified in genome-wide association studies (GWAS) occur in noncoding regions of DNA, underscoring the significance of gene expression and regulation in the mechanisms of disease. Sequencing steady-state RNA in a sample, known as RNA-seq, is free from many of the limitations of previous technologies, such as the dependence on prior knowledge of the organism, as required for microarrays and PCR. RNA‑Seq is a powerful sequencing-based method that captures a full and informative spectrum of gene expression data. Most RNA-seq experiments take a sample of purified RNA, shear it, convert it to cDNA ad sequence on a high-throughput platform, such as the illumina GA/ HiSeq, SOLiD or Roche 454.\nThe following figure outlines the processing pipeline used for detecting differential expression (DE) in RNA-seq.\n\nMapping\n\nThe first step in turning millions of short reads into a quantification of expression is the read mapping or alignment. At its simplest, the task of mapping is to find the unique location where a short read is identical to the reference.\n\nSummarizing mapped reads\n\nHaving obtained genomic locations for as many reads as possible, the next task is to summarize and aggregate reads over some biologically meaningful unit, such as exons, transcripts or genes;\n\nNormalization\n\nNormalization enables accurate comparisons of expression levels between and within samples. Because transcripts have higher read counts (at the same expression level), a common method for within-library normalization is to divide the summarized counts by the length of the gene, such as RPKM (reads per kilobase of exon model per million mapped reads)\n\nDifferential expression\n\nThe goal of a differential expression analysis is to highlight genes that have changed significantly in abundance across experimental conditions. In general, this means taking a table of summarized count data for each library and performing statistical testing between samples of interest.\n\nSystems biology: going beyond gene lists\n\nGene expression studies are laying the groundwork for advances in precision medicine by identifying potential therapeutic biomarkers and drug targets.\nThere is wide scope for integrating the results of RNA-seq data with other sources of biological data to establish a more complete picture of gene regulation\n\nRNA-seq has been used in conjunction with genotyping data to identify genetic loci responsible for variation in gene expression between individuals (expression quantitative trait loci or eQTLs)\nintegration of expression data with transcription factor binding, RNA interference, histone modification and DNA methylation information has the potential for greater understanding of a variety of regulatory mechanisms.\n\n\n\n\nFigure. Overview of the RNA-seq analysis pipeline for detecting differential expression."
  },
  {
    "objectID": "blogs/bioinformatics/09-bio.html#advances-in-rna-seq-techniques",
    "href": "blogs/bioinformatics/09-bio.html#advances-in-rna-seq-techniques",
    "title": "RNA Sequencing",
    "section": "Advances in RNA-Seq techniques",
    "text": "Advances in RNA-Seq techniques\n\nBulk analysis\nSingle cell analysis\n\nSingle-cell sequencing is used to characterize hundreds to tens of thousands of individual cells from a tissue. This method reveals cellular heterogeneity and provides a more comprehensive understanding of tissue composition. S\n\nSpatial analysis\n\nSpatial RNA-Seq provides a previously inaccessible view of the full transcriptome in morphological context. Spatial RNA-Seq methods that retain the precise location of biological molecules in tissue samples can further our understanding of mechanisms in health and disease."
  },
  {
    "objectID": "blogs/bioinformatics/07-bio.html",
    "href": "blogs/bioinformatics/07-bio.html",
    "title": "DNA Sequencing",
    "section": "",
    "text": "Source\nDNA, the acronym for deoxyribonucleic acid, is the genetic code inherited from your mother and facther that exists in almost every cell in your body. The code is made up of about 3 billion bases or letters, consisting of A, T, G, or C (which stand for adenine, thymine, guanine and cytosine), that spell out the personal biological instructions that tell your body how to function over time. You are born with this code, and it doesn’t change. Genotyping is the process of determining which genetic variants an individual possesses, while sequencing is a method used to determine the exact sequence of a certain length of DNA.\nWhen someone has their “DNA sequenced,” it almost always means one of four things ref."
  },
  {
    "objectID": "blogs/bioinformatics/07-bio.html#genotyping-vs.-sequencing",
    "href": "blogs/bioinformatics/07-bio.html#genotyping-vs.-sequencing",
    "title": "DNA Sequencing",
    "section": "Genotyping vs. Sequencing",
    "text": "Genotyping vs. Sequencing\nTo explain the difference between the technologies that read DNA, think of a book. Imagine the string of letters that make up your genetic code are like words on a page, telling a story, chapter by chapter. Genotyping is like reading a few scattered words on a page. Sequencing reads whole sentences, paragraphs and chapters. To sum it up quickly, genotyping gives you small packets of data to compare while sequencing gives you more data, with more meaning and context, today and down the road.\nGenotyping looks for information at specific place in the DNA where we know important data will be. Microarrays (or “arrays”, for short) are just one approach to genotyping but it has paved the way for understanding how common variations in our DNA may be associated with health conditions like diabetes and heart disease. However, while identifying this specific point in the DNA – or “word” on the page – is incredibly important, it also signals to reasearchers that something in that “paragraph” could be even more significant or provide context. This is the equivalent to reading the word “knife” on a page, yet not knowing whether the book is a cookbook or a murder mystery. This is what genotyping is good at: finding what we know today, where we know it will be. That’s a great tactic if you know what you are looking for. But what about what we don’t know? And what about context?\nSequencing looks at all the letters, in the order they are spelled out in your DNA. In some cases, it looks only at a gene, a stretch of DNA that has the instructions for a specific protein. In other cases, the sequencing can look at the entire sequence, all 3 billion or so letters. Through significant advances in technology, we’ve drastically decreased the time and labor costs to do sequencing. The ability to look at the entire sequence of genes faster and cheaper through next generation sequencing (NGS) allow us to see beyond the commonly known variations in your DNA, thus enabling scientists to identify more of the unique variations from person to person. In turn, this leads to deeper discovery about the genetic underpinnings of your health. Without sequencing, we simply miss these nuances to the story. We only know, “there is an apple in this book”. This is what sequencing is good at: uncovering the context, meaning, detail, and granularity."
  },
  {
    "objectID": "blogs/bioinformatics/07-bio.html#wgs-vs-wes",
    "href": "blogs/bioinformatics/07-bio.html#wgs-vs-wes",
    "title": "DNA Sequencing",
    "section": "WGS vs WES",
    "text": "WGS vs WES\n\nExome\nAn exome is the sequence of all the exons in a genome, reflecting the protein-coding portion of a genome. In humans, the exome is about 1.5% of the genome.\nProteins are encoded by genes. And genes consist of two major components, exons and introns. Exons contain the nucleotides that directly encode for proteins, whereas introns are stretches of DNA between the exons and do not encode for proteins. The entire collection of all the exons from all the genes in a genome is called an exome. In the case the human genome, the exome only corresponds to about 1.5% of the genome’s roughly 3 billion nucleotides. Genome scientists have developed laboratory methods that allow them to just sequence a genome’s exome; in other words, just the part of the genome that directly encodes for proteins. So, you will often hear genomicists talk about an exome sequence, which is a very small part of the overall genome (or whole-genome) sequence.\n\n\nWhen to Use Whole-Genome or Whole-Exome Sequencing\nThe complete genomic information within a sample or individual is known as the whole genome. Exons are the genome’s protein-coding regions and are collectively known as the exome. Despite the exome’s relatively small proportion of the whole genome (approximately 2%), exomes encode most known disease-related variants.\n\n\n\n\n\n\n\nUse\nWhen you need to\n\n\n\n\nWGS\nAnalyze the whole genome, including coding, non-coding, and mitochondrial DNA  Discover novel genomic variants (structural, single nucleotide, insertion-deletion, copy number)  Identify previously unknown variants for future targeted studies\n\n\nWES\nIncrease throughput capabilities  Optimize cost per sample  Analyze manageable data sets and maximize data storage\n\n\n\nNote: Both exons and introns are also present in untranslated regions (UTRs) and non-coding RNAs. Misuse of the term exon is problematic, for example, ‘‘whole-exome sequencing’’ technology targets &lt;25% of the human exome, primarily regions that are protein coding. Not all exons are protein coding: Addressing a common misconception"
  },
  {
    "objectID": "blogs/bioinformatics/05-bio.html",
    "href": "blogs/bioinformatics/05-bio.html",
    "title": "Conversion between chr:pos and rsID",
    "section": "",
    "text": "Source: GWASLab\nrsID与染色体位置信息的匹配或转换是生物信息学研究中必不可少的，但却又是十分繁琐的一项步骤，很多同学都在纠结这个问题，本文将总结常用的转换方法，以供参考：\n转换前需要考虑的问题："
  },
  {
    "objectID": "blogs/bioinformatics/05-bio.html#web版本适合个别snp的转换",
    "href": "blogs/bioinformatics/05-bio.html#web版本适合个别snp的转换",
    "title": "Conversion between chr:pos and rsID",
    "section": "Web版本：适合个别SNP的转换",
    "text": "Web版本：适合个别SNP的转换\n\ndbsnp: 可以搜rsID（比如：rs671）获得chr:pos，也可以搜chr:pos（比如：12:112241766）获得rsID；\nSNPnexus: 首先输入用户信息，学术用途是免费的，使用自己的edu邮箱即可;之后选择assembly的版本;选择后可以通过多种方式提交自己要查询的SNP;\n\n输入文件对rsID进行注释或位点转换为rsID\n\n对rsID进行注释：输入文件 snp.txt , 格式如&lt; “dbsnp” rs# &gt; （其他格式的具体描述详见：https://www.snp-nexus.org/v4/guide/）,因为我们只查询位置信息，就不勾选其他数据库，只使用默认的Ensembl；点击submit query后，稍作等待，结果就会显示出来，可以导出为VCF或txt文本格式。\n位点转换为rsID：格式为：&lt; Type Name Position Allele1 Allele2 Strand &gt; # Genomic position data for novel SNPs\n\n\nVEP网页版: 点击launch VEP\n\n可以在input data处直接粘贴要查询的rsID，或是上传文件（点击输入框下方的example，可以查看可用的输入格式）；选择合适数据库，提交后可以看到查询状态；完成后点击View results\n位点转换rsID时，使用如下Ensembl默认的输入格式即可："
  },
  {
    "objectID": "blogs/bioinformatics/05-bio.html#代码工具除biomart其它工具适合大量snp转换",
    "href": "blogs/bioinformatics/05-bio.html#代码工具除biomart其它工具适合大量snp转换",
    "title": "Conversion between chr:pos and rsID",
    "section": "代码工具：除BioMart，其它工具适合大量SNP转换",
    "text": "代码工具：除BioMart，其它工具适合大量SNP转换\n\nBioMart\nVEP command lines\nANNOVAR"
  },
  {
    "objectID": "blogs/bioinformatics/03-bio.html",
    "href": "blogs/bioinformatics/03-bio.html",
    "title": "dx extract_dataset (UKB) for R",
    "section": "",
    "text": "As-Is Software Disclaimer\nThis content in this repository is delivered “As-Is”. Notwithstanding anything to the contrary, DNAnexus will have no warranty, support, liability or other obligations with respect to Materials provided hereunder.\n\nThis notebook demonstrates usage of the dx command extract_dataset for: * Retrieval of Apollo-stored data, as referenced within entities and fields of a Dataset or Cohort object on the platform * Retrieval of the underlying data dictionary files used to generate a Dataset object on the platform\nMIT License applies to this notebook.\n\n\n\n\n\nApp name: JupyterLab with Python, R, Stata, ML ()\nKernel: R\nInstance type: Spark Cluster - mem1_ssd1_v2_x2, 3 nodes\nSnapshot: /.Notebook_snapshots/jupyter_snapshot.gz\nCost: &lt; $0.2\nRuntime: =~ 10 min\nData description: Input for this notebook is a v3.0 Dataset or Cohort object ID\n\n\n\n\nextract_dataset requires dxpy version &gt;= 0.329.0. If running the command from your local environment (i.e. off of the DNAnexus platform), it may be required to also install pandas. For example, pip3 install -U dxpy[pandas]\n#| trusted: true\nsystem(\"pip3 show dxpy\", intern = TRUE)\n\n\n\nQuick note - you will need to read the licenses for the tidyverse in order to make sure whether you and your group are comfortable with the licensing terms.\n#| trusted: true\n#| eval: false\ninstall.packages(c(\"readr\", \"stringr\", \"dplyr\", \"glue\", \"reactable\", \"janitor\", \"remotes\"))\nremotes::install_github(\"laderast/xvhelper\")\n\n\n\n#| trusted: true\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(glue)\nlibrary(reactable)\nlibrary(xvhelper)\n\n\n\n#| trusted: true\n# The referenced Dataset is private and provided only to demonstrate an example input. The user will need to supply a permissible and valid record-id\n\n# Assign project-id of dataset\n# In general, you should use the project id of your UKB project\n\n# Assign dataset record-id\n# record id can be either from a dataset or from a cohort\nprojectid &lt;- \"project-XXXXXX\"\nrid &lt;- \"record-G406j8j0x8kzxv3G08k64gVV\"\n# Assign joint dataset project-id:record-id\ndataset &lt;- glue::glue(\"{projectid}:{rid}\")\n\n\n\nWe’ll use the {glue} package to put our bash commands together for dx extract_dataset, and use system() to execute our bash code.\nglue::glue() has the advantage of not needing to paste() together strings. The string substitution is cleaner.\n#| trusted: true\ncmd &lt;- glue::glue(\"dx extract_dataset {dataset} -ddd\")\n\ncmd\nLet’s execute our command using system() and then we will list the files that result using list.files(). We generate three files in the directory in JupyterLab storage:\n\ndataset_name.codings.csv\ndataset_name.data_dictionary.csv\ndataset_name.entity_dictionary.csv\n\n#| trusted: true\nsystem(cmd)\nlist.files()\n\n\n#| trusted: true\n#codings_file &lt;- system(\"ls *.codings.csv\", intern = TRUE)\ncodings_file &lt;- list.files(pattern=\"*.codings.csv\")\ncodings_df &lt;- read_csv(codings_file, show_col_types = FALSE)\nhead(codings_df)\n#| trusted: true\nentity_dict_file &lt;- system(\"ls *.entity_dictionary.csv\", intern=TRUE)\nentity_dict_df &lt;- read_csv(entity_dict_file, show_col_types = FALSE)\nhead(entity_dict_df)\n\n\n\n\nThe data dictionary is the glue for the entire dataset. It maps:\n\nEntity to Fields\nFields to Codings\nEntity to Entity\n\nWe’ll use the data dictionary to understand how to building our list of fields, and later, we’ll join it to the codings file to build a list of fields and their coded values.\nThere are more columns to the data dictionary, but let’s first see the entity, name, title, and type columns:\n#| trusted: true\n#data_dict_file &lt;- system(\"ls *.data_dictionary.csv\", intern=TRUE)\ndata_dict_file &lt;- list.files(pattern=\"*.data_dictionary.csv\")\ndata_dict_df &lt;- read_csv(data_dict_file, show_col_types = FALSE)\ndata_dict_df &lt;- data_dict_df \n\ndata_dict_df %&gt;%\n        select(entity, name, title, type) %&gt;%\n        head()\n\n\n\nLet’s search for some fields. We want the following fields:\n\nCoffee intake | instance 0\nSex (Gender)\nSmoked cigarette or pipe within last hour | Instance 0\n\nWe can use the {reactable} package to make a searchable table of the data dictionary. This will help in finding fields.\nNote the search box in the top right of the table - when we have many fields, we can use the search box to find fields of interest. Try searching for Coffee intake and see what fields pop up.\n#| trusted: true\ndata_dict_df &lt;- data_dict_df %&gt;%\n    relocate(name, title) %&gt;%\n    mutate(ent_field = glue::glue(\"{entity}.{name}\"))\n\nbasic_data_dict &lt;- data_dict_df |&gt;\n                    select(title, name, entity, ent_field, coding_name, is_multi_select, is_sparse_coding)\n\nreactable::reactable(basic_data_dict, searchable = TRUE)\nAnother strategy for searching fields: we can use grepl within dplyr::filter() to search for fields that match our criteria.\nNote we’re chaining the grepl statements with an OR |.\nWe’re also concatenating entity and name to a new variable, ent_field, which we’ll use when we specify our list of fields.\n#| trusted: true\nfiltered_dict &lt;- data_dict_df %&gt;%\n    filter(grepl(\"Coffee type\", title) | \n           grepl(\"Sex\", title) | \n           grepl(\"Smoked\", title) | \n           grepl(\"Age at recruitment\", title) |\n           grepl(\"main ICD10\", title) |\n           grepl(\"Types of transport\", title)\n          ) %&gt;%\n    arrange(title) \n\nfiltered_dict %&gt;%\n    select(name, title, ent_field)\nLet’s use this subset of fields - we’ll pull the ent_field column, and paste it together into a single comma delimited string using paste:\n#| trusted: true\nfield_list &lt;- filtered_dict %&gt;%\n    pull(ent_field)\n\n#field_list &lt;- field_list[200:210]\nfield_list &lt;- paste(field_list, collapse = \",\")\nfield_list &lt;- paste0(\"participant.eid,\", field_list)\nfield_list\n\n\n\nAgain, we’ll use glue() here for cleaner string substitution.\nWe’ll extract the cohort information to a file called cohort_data.csv and work with this file for the rest of the notebook.\n#| trusted: true\ncohort_template &lt;- \"dx extract_dataset {dataset} --fields {field_list} -o cohort_data.csv\"\ncmd &lt;- glue::glue(cohort_template)\n\ncmd\n\nsystem(cmd)\n\n\nWe’ll see that the retrieved data contains the integer and character codes. These must be decoded (see below):\n#| trusted: true\ndata_df &lt;- read_csv(\"cohort_data.csv\", show_col_types = FALSE)\nhead(data_df)\n\n\n\n\n\nxvhelper is a little R package that will return the actual values of the returned data.\nTo use it, you build a coded_col_df using merge_coding_data_dict() and then translate the categorical columns to values using decode_categories(), and then change the column names to R friendly clean ones using decode_column_names().\nNote that we need to run decode_df() before we run decode_category()\n#| trusted: true\n#| eval: false\n\n#install via remotes::install_github()\n#install.packages(\"remotes\")\nremotes::install_github(\"laderast/xvhelper\")\n\nlibrary(xvhelper)\n#| trusted: true\ncoded_col_df &lt;- xvhelper::merge_coding_data_dict(coding_dict = codings_df, data_dict = data_dict_df)\n\ndecoded &lt;- data_df %&gt;%\n    xvhelper::decode_single(coded_col_df) |&gt;\n    xvhelper::decode_multi_purrr(coded_col_df) |&gt;\n    xvhelper::decode_column_names(coded_col_df, r_clean_names = FALSE)\n    \nhead(decoded)\nlibrary(ggplot2)\n\nggplot(decoded) + \n  aes(x=`Coffee type | Instance 0`) +\n  geom_bar()\ndecoded |&gt;\n  dplyr::select(`Participant ID`, `Diagnoses - main ICD10`) |&gt;\n  tidyr::separate_rows(`Diagnoses - main ICD10`, sep=\"\\\\|\") |&gt;\n  filter(grepl(\"Chapter\", `Diagnoses - main ICD10`)) |&gt;\n  count(`Diagnoses - main ICD10`) |&gt;\n  arrange(desc(n)) |&gt;\n  gt::gt()\ndecoded |&gt;\n  dplyr::select(`Participant ID`, `Diagnoses - main ICD10`) |&gt;\n  tidyr::separate_rows(`Diagnoses - main ICD10`, sep=\"\\\\|\") |&gt;\n  filter(grepl(\"Chapter\", `Diagnoses - main ICD10`)) |&gt;\n  ggplot() + aes(x=`Diagnoses - main ICD10`) + geom_bar() + \n  theme(axis.text.x = element_text(angle = 90))\n#| trusted: true\n#| eval: false\nwrite.csv(decoded, file=\"cohort_decoded.csv\")\n\n\n#| trusted: true\n#| eval: false\nsystem(\"dx upload *.csv --destination /users/tladeras/\")\nSource: GitHub"
  },
  {
    "objectID": "blogs/bioinformatics/03-bio.html#preparing-your-environment",
    "href": "blogs/bioinformatics/03-bio.html#preparing-your-environment",
    "title": "dx extract_dataset (UKB) for R",
    "section": "",
    "text": "App name: JupyterLab with Python, R, Stata, ML ()\nKernel: R\nInstance type: Spark Cluster - mem1_ssd1_v2_x2, 3 nodes\nSnapshot: /.Notebook_snapshots/jupyter_snapshot.gz\nCost: &lt; $0.2\nRuntime: =~ 10 min\nData description: Input for this notebook is a v3.0 Dataset or Cohort object ID\n\n\n\n\nextract_dataset requires dxpy version &gt;= 0.329.0. If running the command from your local environment (i.e. off of the DNAnexus platform), it may be required to also install pandas. For example, pip3 install -U dxpy[pandas]\n#| trusted: true\nsystem(\"pip3 show dxpy\", intern = TRUE)\n\n\n\nQuick note - you will need to read the licenses for the tidyverse in order to make sure whether you and your group are comfortable with the licensing terms.\n#| trusted: true\n#| eval: false\ninstall.packages(c(\"readr\", \"stringr\", \"dplyr\", \"glue\", \"reactable\", \"janitor\", \"remotes\"))\nremotes::install_github(\"laderast/xvhelper\")\n\n\n\n#| trusted: true\nlibrary(dplyr)\nlibrary(readr)\nlibrary(stringr)\nlibrary(glue)\nlibrary(reactable)\nlibrary(xvhelper)\n\n\n\n#| trusted: true\n# The referenced Dataset is private and provided only to demonstrate an example input. The user will need to supply a permissible and valid record-id\n\n# Assign project-id of dataset\n# In general, you should use the project id of your UKB project\n\n# Assign dataset record-id\n# record id can be either from a dataset or from a cohort\nprojectid &lt;- \"project-XXXXXX\"\nrid &lt;- \"record-G406j8j0x8kzxv3G08k64gVV\"\n# Assign joint dataset project-id:record-id\ndataset &lt;- glue::glue(\"{projectid}:{rid}\")\n\n\n\nWe’ll use the {glue} package to put our bash commands together for dx extract_dataset, and use system() to execute our bash code.\nglue::glue() has the advantage of not needing to paste() together strings. The string substitution is cleaner.\n#| trusted: true\ncmd &lt;- glue::glue(\"dx extract_dataset {dataset} -ddd\")\n\ncmd\nLet’s execute our command using system() and then we will list the files that result using list.files(). We generate three files in the directory in JupyterLab storage:\n\ndataset_name.codings.csv\ndataset_name.data_dictionary.csv\ndataset_name.entity_dictionary.csv\n\n#| trusted: true\nsystem(cmd)\nlist.files()\n\n\n#| trusted: true\n#codings_file &lt;- system(\"ls *.codings.csv\", intern = TRUE)\ncodings_file &lt;- list.files(pattern=\"*.codings.csv\")\ncodings_df &lt;- read_csv(codings_file, show_col_types = FALSE)\nhead(codings_df)\n#| trusted: true\nentity_dict_file &lt;- system(\"ls *.entity_dictionary.csv\", intern=TRUE)\nentity_dict_df &lt;- read_csv(entity_dict_file, show_col_types = FALSE)\nhead(entity_dict_df)\n\n\n\n\nThe data dictionary is the glue for the entire dataset. It maps:\n\nEntity to Fields\nFields to Codings\nEntity to Entity\n\nWe’ll use the data dictionary to understand how to building our list of fields, and later, we’ll join it to the codings file to build a list of fields and their coded values.\nThere are more columns to the data dictionary, but let’s first see the entity, name, title, and type columns:\n#| trusted: true\n#data_dict_file &lt;- system(\"ls *.data_dictionary.csv\", intern=TRUE)\ndata_dict_file &lt;- list.files(pattern=\"*.data_dictionary.csv\")\ndata_dict_df &lt;- read_csv(data_dict_file, show_col_types = FALSE)\ndata_dict_df &lt;- data_dict_df \n\ndata_dict_df %&gt;%\n        select(entity, name, title, type) %&gt;%\n        head()\n\n\n\nLet’s search for some fields. We want the following fields:\n\nCoffee intake | instance 0\nSex (Gender)\nSmoked cigarette or pipe within last hour | Instance 0\n\nWe can use the {reactable} package to make a searchable table of the data dictionary. This will help in finding fields.\nNote the search box in the top right of the table - when we have many fields, we can use the search box to find fields of interest. Try searching for Coffee intake and see what fields pop up.\n#| trusted: true\ndata_dict_df &lt;- data_dict_df %&gt;%\n    relocate(name, title) %&gt;%\n    mutate(ent_field = glue::glue(\"{entity}.{name}\"))\n\nbasic_data_dict &lt;- data_dict_df |&gt;\n                    select(title, name, entity, ent_field, coding_name, is_multi_select, is_sparse_coding)\n\nreactable::reactable(basic_data_dict, searchable = TRUE)\nAnother strategy for searching fields: we can use grepl within dplyr::filter() to search for fields that match our criteria.\nNote we’re chaining the grepl statements with an OR |.\nWe’re also concatenating entity and name to a new variable, ent_field, which we’ll use when we specify our list of fields.\n#| trusted: true\nfiltered_dict &lt;- data_dict_df %&gt;%\n    filter(grepl(\"Coffee type\", title) | \n           grepl(\"Sex\", title) | \n           grepl(\"Smoked\", title) | \n           grepl(\"Age at recruitment\", title) |\n           grepl(\"main ICD10\", title) |\n           grepl(\"Types of transport\", title)\n          ) %&gt;%\n    arrange(title) \n\nfiltered_dict %&gt;%\n    select(name, title, ent_field)\nLet’s use this subset of fields - we’ll pull the ent_field column, and paste it together into a single comma delimited string using paste:\n#| trusted: true\nfield_list &lt;- filtered_dict %&gt;%\n    pull(ent_field)\n\n#field_list &lt;- field_list[200:210]\nfield_list &lt;- paste(field_list, collapse = \",\")\nfield_list &lt;- paste0(\"participant.eid,\", field_list)\nfield_list\n\n\n\nAgain, we’ll use glue() here for cleaner string substitution.\nWe’ll extract the cohort information to a file called cohort_data.csv and work with this file for the rest of the notebook.\n#| trusted: true\ncohort_template &lt;- \"dx extract_dataset {dataset} --fields {field_list} -o cohort_data.csv\"\ncmd &lt;- glue::glue(cohort_template)\n\ncmd\n\nsystem(cmd)\n\n\nWe’ll see that the retrieved data contains the integer and character codes. These must be decoded (see below):\n#| trusted: true\ndata_df &lt;- read_csv(\"cohort_data.csv\", show_col_types = FALSE)\nhead(data_df)"
  },
  {
    "objectID": "blogs/bioinformatics/03-bio.html#decoding-columns-with-xvhelper",
    "href": "blogs/bioinformatics/03-bio.html#decoding-columns-with-xvhelper",
    "title": "dx extract_dataset (UKB) for R",
    "section": "",
    "text": "xvhelper is a little R package that will return the actual values of the returned data.\nTo use it, you build a coded_col_df using merge_coding_data_dict() and then translate the categorical columns to values using decode_categories(), and then change the column names to R friendly clean ones using decode_column_names().\nNote that we need to run decode_df() before we run decode_category()\n#| trusted: true\n#| eval: false\n\n#install via remotes::install_github()\n#install.packages(\"remotes\")\nremotes::install_github(\"laderast/xvhelper\")\n\nlibrary(xvhelper)\n#| trusted: true\ncoded_col_df &lt;- xvhelper::merge_coding_data_dict(coding_dict = codings_df, data_dict = data_dict_df)\n\ndecoded &lt;- data_df %&gt;%\n    xvhelper::decode_single(coded_col_df) |&gt;\n    xvhelper::decode_multi_purrr(coded_col_df) |&gt;\n    xvhelper::decode_column_names(coded_col_df, r_clean_names = FALSE)\n    \nhead(decoded)\nlibrary(ggplot2)\n\nggplot(decoded) + \n  aes(x=`Coffee type | Instance 0`) +\n  geom_bar()\ndecoded |&gt;\n  dplyr::select(`Participant ID`, `Diagnoses - main ICD10`) |&gt;\n  tidyr::separate_rows(`Diagnoses - main ICD10`, sep=\"\\\\|\") |&gt;\n  filter(grepl(\"Chapter\", `Diagnoses - main ICD10`)) |&gt;\n  count(`Diagnoses - main ICD10`) |&gt;\n  arrange(desc(n)) |&gt;\n  gt::gt()\ndecoded |&gt;\n  dplyr::select(`Participant ID`, `Diagnoses - main ICD10`) |&gt;\n  tidyr::separate_rows(`Diagnoses - main ICD10`, sep=\"\\\\|\") |&gt;\n  filter(grepl(\"Chapter\", `Diagnoses - main ICD10`)) |&gt;\n  ggplot() + aes(x=`Diagnoses - main ICD10`) + geom_bar() + \n  theme(axis.text.x = element_text(angle = 90))\n#| trusted: true\n#| eval: false\nwrite.csv(decoded, file=\"cohort_decoded.csv\")\n\n\n#| trusted: true\n#| eval: false\nsystem(\"dx upload *.csv --destination /users/tladeras/\")\nSource: GitHub"
  },
  {
    "objectID": "blogs/bioinformatics/01-bio.html",
    "href": "blogs/bioinformatics/01-bio.html",
    "title": "Calculate Polygenic Risk Score on UKB",
    "section": "",
    "text": "use dx command line to log in the account in UKB RAP\n\ndx login\n\nrun the following shell command run swiss-army-knife of UKB RAP (extract genotyping data with some SNPs)\n\n# pull genotyping data from UKB using UKB RAP\nsh 01-pull-snps-imp37.sh\n\ndescription of some files\n\ncodes in 01-pull-snps-imp37.sh\n\n# shell command lines\nimp_file_dir=\"/Bulk/Imputation/UKB imputation from genotype\"\ndata_field=\"ukb22828\"\nrsidlist=\"multiDis_SNP.txt\"\ngenetic_data_dir=\"/Genetic_data/\"\n\nfor i in $(seq 1 1 22); do\n    run_snps=\"bgenix -g ${data_field}_c${i}_b0_v3.bgen -incl-rsids ${rsidlist} &gt; chr_${i}.bgen\"\n\n    dx run swiss-army-knife -iin=\"${imp_file_dir}/${data_field}_c${i}_b0_v3.bgen\" \\\n        -iin=\"${imp_file_dir}/${data_field}_c${i}_b0_v3.sample\" \\\n        -iin=\"${imp_file_dir}/${data_field}_c${i}_b0_v3.bgen.bgi\" \\\n        -iin=\"${genetic_data_dir}/${rsidlist}\" \\\n        -icmd=\"${run_snps}\" --tag=\"SelectSNPs\" --instance-type \"mem2_ssd2_v2_x16\" \\\n        --destination=\"${project}:${genetic_data_dir}\" --brief --yes\ndone\n\npart text in multiDis_SNP.txt\n\nrs1060743\nrs1532278\nrs3865444\nrs6656401\nrs7232\nthe following analysis are run in local machine\n\ncombine multiple bgen files\n\n# combine multiple bgen files\ncat-bgen -g ukb22828_chr_1.bgen -g ukb22828_chr_2.bgen -g ukb22828_chr_3.bgen \\\n-g ukb22828_chr_4.bgen -g ukb22828_chr_5.bgen -g ukb22828_chr_6.bgen -g ukb22828_chr_7.bgen \\\n-g ukb22828_chr_8.bgen -g ukb22828_chr_9.bgen -g ukb22828_chr_10.bgen -g ukb22828_chr_11.bgen \\\n-g ukb22828_chr_12.bgen -g ukb22828_chr_13.bgen -g ukb22828_chr_14.bgen -g ukb22828_chr_15.bgen \\\n-g ukb22828_chr_16.bgen -g ukb22828_chr_17.bgen -g ukb22828_chr_18.bgen -g ukb22828_chr_19.bgen \\\n-g ukb22828_chr_20.bgen -g ukb22828_chr_21.bgen -g ukb22828_chr_22.bgen -og initial_chr.bgen -clobber\n\nwrite index file (.bgen.bgi)\n\n# write index file .bgen.bgi\nbgenix -g initial_chr.bgen -index -clobber\n\nextract SNPs for single disease (extract SNPs of multiple diseases using the above codes)\n\n# extract SNPs\ndisease_name=\"J10_INFLUPNEU\"\nbgenix -g initial_chr.bgen -incl-rsids snps_${disease_name}.txt &gt; ${disease_name}.bgen\n\nwrite index file for disease_name .bgen.bgi\n\n# write index file for disease_name .bgen.bgi\nbgenix -g ${disease_name}.bgen -index -clobber\n\nconvert to plinkformat\n\n# convert to plinkformat\nplink2 --bgen ${disease_name}.bgen ref-first --sample ukb22828_c1_b0_v3.sample --freq --maf 0.01 --make-pgen --sort-vars --out ${disease_name}\n\ncalculate prs score\n\n# calculate prs score {-iin pfile, score_file}\nplink2 --pfile ${disease_name} --score ${disease_name}_scorefile.txt no-mean-imputation list-variants cols=maybefid,nallele,denom,dosagesum,scoreavgs,scoresums --out ${disease_name}_prsout.txt\n\n${disease_name}_scorefile.txt (summary statistics from the disease)\n\nrsid effect_allele Beta\nrs146800061 G 0.362028\nrs13088725 T 0.0946831\nrs11726368 T 0.133672\nrs117066711 C 0.388579\nrs9507345 G 0.17304\nrs2248248 G 0.14556\nrs140194168 C 0.367733\nrs28655344 C 0.14604\nrs28627461 A 0.0763166\nrs34772569 C 0.243625\nrs2837113 G 0.0779496\n\npart of results\n\neid   IID ALLELE_CT   DENOM   NAMED_ALLELE_DOSAGE_SUM SCORE1_AVG  SCORE1_SUM\n3274335   3274335 54  54  29.886  0.0418474   2.25976\n4217650   4217650 54  54  28.941  0.0435369   2.35099\n3332919   3332919 54  54  32.314  0.0430303   2.32363\n2602284   2602284 54  54  31.773  0.0430288   2.32356\n5418540   5418540 54  54  32.965  0.0450697   2.43377\n2806429   2806429 54  54  31.004  0.0415628   2.24439\n5922336   5922336 54  54  28.98   0.041512    2.24165\n4299370   4299370 54  54  32.745  0.0441643   2.38487\n5381487   5381487 54  54  31.012  0.0411954   2.22455\n1811259   1811259 54  54  31.969  0.042009    2.26848\n4164188   4164188 54  54  35.588  0.0456951   2.46753\nExample Polygenic risk score files\n\nThis example file is for type 2 Diabetes as used in PMID: 31232720\nThis example files follows the format of PMID: 35251129\nA seconf example file fro pancreatic cancer based on the file from the PGS Catalog. PGS Catalog Pancreatic cancer PRS\n\n\nSource: GitHub"
  },
  {
    "objectID": "blogs/bioinformatics/02-bio.html",
    "href": "blogs/bioinformatics/02-bio.html",
    "title": "Use Jupyter Spark to Extract Phenotypic Data from the UKB Database",
    "section": "",
    "text": "import databricks.koalas as ks\nimport dxpy\nimport dxdata\nimport pandas as pd\nimport pyspark\nimport re\n\nInitialize Spark\n\n# Initialize Spark\n# Spark initialization (Done only once; do not rerun this cell unless you select Kernel -&gt; Restart kernel).\nsc = pyspark.SparkContext()\nspark = pyspark.sql.SparkSession(sc)\n\nAutomatically discover dispensed dataset ID and load the dataset\n\n# Automatically discover dispensed dataset ID and load the dataset\ndispensed_dataset = dxpy.find_one_data_object(\n    typename=\"Dataset\", \n    name=\"app*.dataset\", \n    folder=\"/\", \n    name_mode=\"glob\")\ndispensed_dataset_id = dispensed_dataset[\"id\"]\ndataset = dxdata.load_dataset(id=dispensed_dataset_id)\nparticipant = dataset['participant']\nfield_ids = ['31', '21022', '41270']\n# for i in range(0,259):\n#     field_ids.append('41280_a'+str(i))\nprint(field_ids)\n\ngrab all field names\n\n# This function is used to grab all field names (e.g. \"p&lt;field_id&gt;_iYYY_aZZZ\") of a list of field IDs\ndef fields_for_id(field_id):\n    from distutils.version import LooseVersion\n    field_id = str(field_id)\n    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n    return sorted(fields, key=lambda f: LooseVersion(f.name))\n#field_ids = ['31', '22001', '22006', '22019', '22021', '21022']\nfields = [participant.find_field(name='eid')] + [participant.find_field(name='p20160_i0')] + [fields_for_id(f)[0] for f in field_ids]\nfield_description = pd.DataFrame({\n    'Field': [f.name for f in fields],\n    'Title': [f.title for f in fields],\n    'Coding': [f.coding.codes if f.coding is not None else '' for f in fields ]\n })\nfield_description\n\nload cohort created by UKB RAP\n\nsamples = dxdata.load_cohort(\"/Cohort/all_participants\") \n\nretrieve data for fields\n\nsamples_df = participant.retrieve_fields(fields = fields, filter_sql = samples.sql, engine=dxdata.connect()).to_koalas()\n\nexport data\n\ntype(samples_df)\ndf_phenotype = samples_df.to_pandas()\ndf_phenotype.shape\ndf_phenotype.head()\nsamples_df.to_csv('all_samples.txt', sep='\\t', na_rep='NA', index=False, quoting=3)\n\nupload (save) codes and results\n\n%%bash -s \"/phenotype_data/\"\ndx upload all_samples_.txt -p --path $1 --brief\n%%bash -s \"/code/\"\ndx upload big_allsamples.ipynb -p --path $1 --brief\nSource"
  },
  {
    "objectID": "blogs/bioinformatics/04-bio.html",
    "href": "blogs/bioinformatics/04-bio.html",
    "title": "Running local LD operations using ieugwasr",
    "section": "",
    "text": "We have tried to provide useful cloud-based functionality for many operations, including relatively demanding LD operations. If you are running a large number of LD operations, we request that you think about performing those locally rather than through the API. We have tried to write the software to enable this to work seamlessly. Some examples below.\nLD operations available on the OpenGWAS API"
  },
  {
    "objectID": "blogs/bioinformatics/04-bio.html#ld-clumping",
    "href": "blogs/bioinformatics/04-bio.html#ld-clumping",
    "title": "Running local LD operations using ieugwasr",
    "section": "LD clumping",
    "text": "LD clumping\nThe API has a wrapper around plink version 1.90 and can use it to perform clumping with an LD reference panel from 1000 genomes reference data.\na &lt;- tophits(id=\"ieu-a-2\", clump=0)\nb &lt;- ld_clump(\n    dplyr::tibble(rsid=a$name, pval=a$p, id=a$id)\n)\nThere are 5 super-populations that can be requested via the pop argument. By default this will use the Europeans subset (EUR super-population). The reference panel has INDELs removed and only retains SNPs with MAF &gt; 0.01 in the selected population.\nNote that you can perform the same operation locally if you provide a path to plink and a bed/bim/fam LD reference dataset.\nTo get a path to plink you can do the following:\ndevtools::install_github(\"explodecomputer/genetics.binaRies\")\ngenetics.binaRies::get_plink_binary()\nTo get the same LD reference dataset that is used by the API, you can download it directly from here:\nhttp://fileserve.mrcieu.ac.uk/ld/1kg.v3.tgz\nThis contains an LD reference panel for each of the 5 super-populations in the 1000 genomes reference dataset. e.g. for the European super population it has the following files:\n\nEUR.bed\nEUR.bim\nEUR.fam\n\nNow supposing in R you have a dataframe, dat, with the following columns:\n\nrsid\npval\ntrait_id\n\nto perform clumping, just do the following:\nld_clump(\n    dplyr::tibble(rsid=dat$rsid, pval=dat$pval, id=dat$trait_id),\n    plink_bin = genetics.binaRies::get_plink_binary(),\n    bfile = \"/path/to/reference/EUR\"\n)"
  },
  {
    "objectID": "blogs/bioinformatics/04-bio.html#ld-matrix",
    "href": "blogs/bioinformatics/04-bio.html#ld-matrix",
    "title": "Running local LD operations using ieugwasr",
    "section": "LD matrix",
    "text": "LD matrix\nSimilarly, a matrix of LD r values can be generated using\nld_matrix(b$variant)\nThis uses the API by default but is limited to only 500 variants. You can use, instead, local plink and LD reference data in the same manner as in the ld_clump function, e.g.\nld_matrix(\n    dat$rsid,\n    plink_bin = genetics.binaRies::get_plink_binary(),\n    bfile = \"/path/to/reference/EUR\"\n)"
  },
  {
    "objectID": "blogs/bioinformatics/04-bio.html#ld-proxies",
    "href": "blogs/bioinformatics/04-bio.html#ld-proxies",
    "title": "Running local LD operations using ieugwasr",
    "section": "LD proxies",
    "text": "LD proxies\nTo automatically extract variants from a dataset, and search for LD proxies when a requested variant is not present in the dataset, please look at the options available in the gwasvcf package:\nhttps://mrcieu.github.io/gwasvcf/articles/guide.html#ld-proxies-1\nSource: GitHub"
  },
  {
    "objectID": "blogs/bioinformatics/06-bio.html",
    "href": "blogs/bioinformatics/06-bio.html",
    "title": "MR Using Genetics to Study Behaviors and Environments that Cause Disease",
    "section": "",
    "text": "Source: CDC\nMendelian randomization studies examine how certain behaviors, environments, or other factors lead to specific health outcomes by looking at genetic differences that affect the way people’s bodies react to the behavior, environment, or other factor."
  },
  {
    "objectID": "blogs/bioinformatics/06-bio.html#why-use-mendelian-randomization",
    "href": "blogs/bioinformatics/06-bio.html#why-use-mendelian-randomization",
    "title": "MR Using Genetics to Study Behaviors and Environments that Cause Disease",
    "section": "Why use Mendelian Randomization?",
    "text": "Why use Mendelian Randomization?\nAn important part of public health research is looking at whether behaviors, environments, and other factors, which can sometimes be changed, make people more likely to get certain diseases. However, showing that these factors cause a specific disease presents challenges.\nIn some studies, researchers compare people with and without a certain disease to look for behaviors, environments, and other factors that are linked to disease. In other studies, researchers compare health outcomes in people with and without a specific behavior, environment, or other factor to see if one group is more or less likely to develop a disease. The researchers are not able to control who has which behavior, environment, or other factor. Accurately measuring some behaviors, such as how much a person smokes or uses alcohol, can be difficult. In some cases, one factor is linked to other factors, and figuring out which one is the cause can be hard. For example, people who have higher alcohol use also tend to smoke more, so a link found between alcohol use and a health outcome might actually be due to smoking causing the outcome. Some factors can be present because of the disease itself, meaning that they are caused by the disease rather than leading to it. For example, as people get sick, they might tend to smoke less, so that decreased smoking might be associated with worsening of disease, even though in reality the decrease in smoking does not make the disease worse. These issues can mean that findings from one study are not repeated in later studies or that different studies can have findings that do not agree.\nThe gold standard for studying whether behavioral, environmental, or other factors cause disease is randomized controlled trials (RCTs). RCTs involve randomly assigning participants to one of two or more groups that differ by one factor. For example, those in a group either do or do not perform a certain behavior, are or are not exposed to a specific environmental factor, or do or do not receive a treatment. However, RCTs are costly, take a long time, and are not always possible due to ethical or other concerns, such as those related to assigning a group of people to perform an unhealthy behavior. RCTs are often used to confirm findings from other studies, and due to the cost of conducting RCTs, it is important to have the best evidence available from other study designs prior to starting an RCT. Mendelian randomization studies can help provide this evidence."
  },
  {
    "objectID": "blogs/bioinformatics/06-bio.html#how-does-mendelian-randomization-work",
    "href": "blogs/bioinformatics/06-bio.html#how-does-mendelian-randomization-work",
    "title": "MR Using Genetics to Study Behaviors and Environments that Cause Disease",
    "section": "How does Mendelian Randomization work?",
    "text": "How does Mendelian Randomization work?\n“Mendelian randomization” is based on the fact that every person randomly inherits one of the two versions of every gene that each of their parents has. Each genetic difference is inherited independently, meaning that one genetic difference a person inherits does not influence which other genetic differences they inherit. Mendelian randomization studies take advantage of this fact to look for links between health outcomes and genetic differences that have a similar effect as the randomly assigned behaviors, environments, or other factors in RCTs but are not subject to the influence of other factors. For example, a Mendelian randomization study could look at genetic differences that affect how people’s bodies break down a toxin that is thought to be linked to a disease. If people with the genetic difference that makes their bodies less able to break down the toxin show higher rates of disease, this provides evidence that the toxin causes the disease. This effect would only be seen for people exposed to the toxin, and the genetic difference would have no effect on disease risk in those not exposed to the toxin. Mendelian randomization can be used to look at whether levels of a substance found naturally in a person’s body is linked to disease. Measuring the levels themselves can be a problem because the levels can change in response to other factors that could be related to the disease, such as diet, smoking, or alcohol use. However, if genetic differences that affect the levels are linked to the disease, that provides evidence that the substance is related to the disease, as shown in the example for high cholesterol and heart disease (see textbox).\n\nEXAMPLE: Mendelian Randomization Supports the Role of High Cholesterol and Heart Disease\nScientists observed that people with higher blood levels of low-density lipoprotein (LDL) cholesterol are more likely to have coronary artery disease. While this suggested that high blood LDL levels could lead to coronary artery disease, other explanations were possible.\n\nFor example, an unknown factor could cause both high LDL levels and make people more likely to develop coronary artery disease. This would make it seem like high LDL cholesterol levels caused coronary artery disease when the unknown factor was the actual cause.\n\nUsing a Mendelian randomization approach, scientists could look at genetic differences that affect LDL cholesterol levels to see if these variants make people more likely to develop coronary artery disease. The unknown factor would not influence which genetic difference is inherited.\n\nFor example, people with familial hypercholesterolemia have genetic changes that increase their blood levels of LDL cholesterol. These genetic changes are linked to an increased risk of coronary artery disease, which provides evidence that high LDL levels can cause heart disease.\n\nThese genetic changes are present from birth, before the development of coronary artery disease, providing evidence that high LDL levels lead to coronary artery disease, rather than coronary artery disease leading to high LDL."
  },
  {
    "objectID": "blogs/bioinformatics/06-bio.html#mendelian-randomization-and-public-health",
    "href": "blogs/bioinformatics/06-bio.html#mendelian-randomization-and-public-health",
    "title": "MR Using Genetics to Study Behaviors and Environments that Cause Disease",
    "section": "Mendelian Randomization and Public Health",
    "text": "Mendelian Randomization and Public Health\nMendelian randomization is one of many examples of how genetic approaches can help increase our understanding of the causes of disease. This approach has not been fully utilized in public health so far and finding genetic differences that result in effects similar to behaviors, environments, or other factors of interest can be challenging. In addition, showing that the genetic difference results in the health outcome through its effect on the behavior, environment, or other factor, not through a different pathway, can be challenging. Nonetheless, Mendelian randomization promises to be a helpful tool for future public health research."
  },
  {
    "objectID": "blogs/bioinformatics/08-bio.html",
    "href": "blogs/bioinformatics/08-bio.html",
    "title": "What is Epigenetics?",
    "section": "",
    "text": "Source: CDC\nYour genes play an important role in your health, but so do your behaviors and enviroment, such as what you eat and how physically active you are. Epigenetics is the study of how your behaviors and enviroment can cause changes that affect the way your genes work. Unlike genetic changes, epigenetic changes are reversible and do not change your DNA sequence, but they can change how your body reads a DNA sequence.\nGene expression refers to how often or when proteins are created from the instructions within your genes. While genetic changes can alter which protein is made, epigenetic changes affect gene expression to turn genes “on” and “off”. Since your enviroment and behaviors, such as diet and exercise, can result in epigenetic changes, it is easy to see the connection between your genes and your behaviors and environment."
  },
  {
    "objectID": "blogs/bioinformatics/08-bio.html#how-does-epigenetics-work",
    "href": "blogs/bioinformatics/08-bio.html#how-does-epigenetics-work",
    "title": "What is Epigenetics?",
    "section": "How Does Epigenetics Work?",
    "text": "How Does Epigenetics Work?\nEpigenetic changes affect gene expression in different ways. Types of epigenetic changes include:\n\nDNA Methylation\n\nDNA methylation works by adding a chemical group to DNA. Typycally, this group is added to specific places on the DNA, where it blocks the proteins that attach to DNA to “read” the gene. This chemical group can be removed through a process called demethylation. Typically, methylation turns genes “off” and demethylation turns genes “on”.\n\nHistone Modification\n\nDNA wraps around proteins called histones. When histones are tightly packed together, proteins that ‘read’ the gene cannot access the DNA as easily, so the gene is turn “off”. When histones are loosely packed, more DNA is exposed or nor wrapped around a histone and can be accessed by proteins that ‘read’ the gene, so the gene is turned “on”. Chemical groups can be added or removed from histones to make the histones more tightly or loosely packed, turning genes “off” or “on”.\n\nNon-coding RNA\n\nYour DNA is used as instructions for making coding and non-coding RNA. Coding RNA is used to make proteins. Non-coding RNA helps control gene expression by attaching to coding RNA, along with certain proteins, to break down the coding RNA so that it cannot be used to make proteins. Non-coding RNA may also recruit proteins to modify histones to turn genes “on” or “off”."
  },
  {
    "objectID": "blogs/bioinformatics/08-bio.html#how-can-your-epigenetics-changes",
    "href": "blogs/bioinformatics/08-bio.html#how-can-your-epigenetics-changes",
    "title": "What is Epigenetics?",
    "section": "How Can Your Epigenetics Changes?",
    "text": "How Can Your Epigenetics Changes?\nYour epigenetics change as you age, both as part of normal development and aging and in response to your behaviors and environment.\n\nEpigenetics and Development\n\nEpigenetic changes begin before you are born. All your cells have the same genes but look and act differently. As you grow and develop, epigenetics helps determine which function a cell will have, for example, whether it will become a heart cell, nerve cell, or skin cell.\nExample: Nerve Cell VS. Muscle Cell\nYour muscle cells and nerve cells have the same DNA but work differently. A nerve cell transports information to other cells in your body. A muscle cell has a structure that aids in your body’s ability to move. Epigenetics allows the muscle cell to turn “on” genes to make proteins important for its job and turn “off” genes important for a nerve cell’s job.\n\nEpigenetics and Age\n\nYour epigenetics change throughout your life. Your epigenetics at birth is not the same as your epigenetics during childhood or adulthood.\nEXAMPLE: STUDY OF NEWBORN VS. 26-YEAR-OLD VS. 103-YEAR-OLD\nDNA methylation at millions of sites were measured in a newborn, 26-year-old, and 103-year-old. The level of DNA methylation decreases with age. A newborn had the highest DNA methylation, the 103-year-old had the lowest DNA methylation, and the 26-year-old had a DNA methylation level between the newborn and 103-year-old.\n\nEpigenetics and Reversibility\n\nNot all epigenetic changes are permanent. Some epigenetic changes can be added or removed in response to changes in behavior or environment.\nEXAMPLE: SMOKERS VS. NON-SMOKERS VS. FORMER SMOKERS Smoking can result in epigenetic changes. For example, at certain parts of the AHRR gene, smokers tend to have less DNA methylation than non-smokers. The difference is greater for heavy smokers and long-term smokers. After quitting smoking, former smokers can begin to have increased DNA methylation at this gene. Eventually, they can reach levels similar to those of non-smokers. In some cases, this can happen in under a year, but the length of time depends on how long and how much someone smoked before quitting."
  },
  {
    "objectID": "blogs/bioinformatics/08-bio.html#epigenetics-and-health",
    "href": "blogs/bioinformatics/08-bio.html#epigenetics-and-health",
    "title": "What is Epigenetics?",
    "section": "Epigenetics and Health",
    "text": "Epigenetics and Health\nEpigenetic changes can affect your health in different ways:\n\nInfection\n\nGerms can change your epigenetics to weaken your immune system. This helps the germ survive.\nEXAMPLE: MYCOBACTERIUM TUBERCULOSIS\nMycobacterium tuberculosis causes tuberculosis. Infections with these germs can cause changes to histones in some of your immune cells that result in turning “off” the IL-12B gene. Turning “off” the IL-12B gene weakens your immune system and improves the survival of Mycobacterium tuberculosis\n\nCancer\n\nCertain mutations make you more likely to develop cancer. Likewise, some epigenetic changes increase your cancer risk. For example, having a mutation in the BRCA1 gene that prevents it from working properly makes you more likely to get breast and other cancers. Similarly, increased DNA methylation that results in decreased BRCA1 gene expression raises your risk for breast and other cancers.While cancer cells have increased DNA methylation at certain genes, overall DNA methylation levels are lower in cancer cells compared with normal cells. Different types of cancer that look alike can have different DNA methylation patterns. Epigenetics can be used to help determine which type of cancer a person has or can help to find hard to detect cancers earlier. Epigenetics alone cannot diagnose cancer, and cancers would need to be confirmed with further screening tests.\nEXAMPLE: COLORECTAL CANCER\nColorectal cancers have abnormal methylation at DNA regions near certain genes, which affects expression of these genes. Some commercial colorectal cancer screening tests use stool samples to look for abnormal DNA methylation levels at one or more of these DNA regions. It is important to know that if the test result is positive or abnormal, a colonoscopy test is needed to complete the screening process.\n\nNutrition During Pregnancy\n\nA pregnant woman’s environment and behavior during pregnancy, such as whether she eats healthy food, can change the baby’s epigenetics. Some of these changes can remain for decades and might make the child more likely to get certain diseases.\nEXAMPLE: DUTCH HUNGER WINTER FAMINE (1944-1945)\nPeople whose mothers were pregnant with them during the famine were more likely to develop certain diseases such as heart disease, schizophrenia, and type 2 diabetes. Around 60 years after the famine, researchers looked at methylation levels in people whose mothers were pregnant with them during the famine. These people had increased methylation at some genes and decreased methylation at other genes compared with their siblings who were not exposed to famine before their birth. These differences in methylation could help explain why these people had an increased likelihood for certain diseases later in life."
  },
  {
    "objectID": "blogs/statistics/02-stats.html",
    "href": "blogs/statistics/02-stats.html",
    "title": "Confounding Bias",
    "section": "",
    "text": "Source: Catalog of Bias\nA distortion that modifies an association between an exposure and an outcome because a factor is independently associated with the exposure and the outcome."
  },
  {
    "objectID": "blogs/statistics/02-stats.html#background",
    "href": "blogs/statistics/02-stats.html#background",
    "title": "Confounding Bias",
    "section": "Background",
    "text": "Background\nThe importance of confounding is that it suggests an association where none exists or masks a true association:\n\nFigure 1. The principle of confounding; the confounder makes the exposure more likely and in some way independently modifies the outcome, making it appear that there is an association between the exposure and the outcome when there is none, or masking a true association\nIt commonly occurs in observational studies, but can also occur in randomized studies, especially, but not only, if they are poorly designed.\nFor example, if by chance more elderly people are randomized to an active intervention than to placebo, and if age is independently more likely to be associated with a beneficial outcome, the intervention may falsely appear to be beneficial.\nBecause observational studies are not randomized to ensure equivalent groups for comparison (or to eliminate imbalances due to chance), confounders are common.\nIt is possible to reduce the effects of known possible confounders by analysing the data statistically in ways that allow for them. However, there will always be the possibility of unknown confounders, which cannot be taken into account. It is therefore not uncommon for the results of observational studies to be overturned when subsequent randomised trials do not confirm the results of the observational studies."
  },
  {
    "objectID": "blogs/statistics/02-stats.html#examples",
    "href": "blogs/statistics/02-stats.html#examples",
    "title": "Confounding Bias",
    "section": "Examples",
    "text": "Examples\nIn a seminal example, early findings on the supposed beneficial effect of hormone replacement therapy in cardiovascular disease were reversed when studies that adjusted for socioeconomic status or education were accounted for; there was a reduced risk among studies that did not adjust for these factors, suggesting confounding.\nIn retrospective, non-randomized studies of patients taking digoxin there were increased death rates, even after adjustment for plausible confounders; however, in a prospective randomized study, mortality was not increased. This suggests that the observational data were subject to confounding. For example, it was likely that those who had been taking digoxin in the observational studies were sicker and therefore more likely to die.\nOther comparisons within a study may give information about the potential role of confounders. For example, in a register-based retrospective nationwide cohort study of 848,786 pregnancies, using the Danish Medical Birth Registry, there was an apparent association between the use of selective serotonin reuptake inhibitors (SSRIs) during pregnancy in 4183 women and an increased risk of certain congenital defects. However, multivariable logistic regression models reduced the significance of an association; furthermore, there were similar risks in a group of women who had stopped taking SSRIs during pregnancy, strongly suggesting that the apparent association was due to an unidentified confounder. Analysis of the effect of dose as a continuous variable showed that there was no dose-response association, further evidence that there was no true association."
  },
  {
    "objectID": "blogs/statistics/02-stats.html#impact",
    "href": "blogs/statistics/02-stats.html#impact",
    "title": "Confounding Bias",
    "section": "Impact",
    "text": "Impact\nIn a systematic review of epidemiological (case-control and cohort) studies of the effectiveness of statins in reducing the risk of Parkinson’s disease (PD), Bykov and colleagues investigated the impact of confounding. Six of 10 included studies collectively showed a protective effect of statins (relative risk 0.75; 95% CI: 0.60–0.92); however, these studies did not adjust (control) for serum cholesterol concentrations, which are inversely related to the risk of PD. In the four studies that did control for cholesterol, the beneficial effect of statins fell by 28% (7–65%) and shifted the point estimate to a non-significant harmful effect of statins (RR 1.04; 0.68–1.59)."
  },
  {
    "objectID": "blogs/statistics/02-stats.html#preventive-steps",
    "href": "blogs/statistics/02-stats.html#preventive-steps",
    "title": "Confounding Bias",
    "section": "Preventive steps",
    "text": "Preventive steps\nRandomization is the best way to reduce the risk of confounding. However, it may not be enough, particularly when it is anticipated that imbalances in prognostic factors may occur despite randomization, or when imbalances occur by chance. Stratification and statistical adjustment can reduce the risk of confounding in such cases.\nAn extension of this is the use of propensity scores, in which potential confounders are used to build a statistical model that assigns to each person a number called their propensity score: the people with high scores are more likely to have certain confounders, and those with low scores are less likely. The use of propensity scores in a study of metformin showed that the risk of cancers is lower in metformin users. But the randomized trial evidence, as far as it goes, shows no convincing evidence that metformin has any effect, illustrating the difficulty in adequately controlling for confounding.\nOn the other hand, a very large effect size can outweigh the combined effects of plausible confounders. Even if plausible confounders have not been ruled out by the design of the study, a large observed effect can swamp the combined effects of the confounders. For example, the observable effects of general anaesthesia are unlikely to be accountable by confounding, placebo effects, or any kinds of biases; in such cases, randomized trials may not even be necessary. In observational studies associations have to be dramatic if one wants to be confident that plausible confounders have been ruled out; this is true of both beneficial and harmful associations."
  },
  {
    "objectID": "blogs/statistics/04-stats.html",
    "href": "blogs/statistics/04-stats.html",
    "title": "Collider bias",
    "section": "",
    "text": "Source: Catalogue of Bias\nA distortion that modifies an association between an exposure and outcome, caused by attempts to control for a common effect of the exposure and outcome."
  },
  {
    "objectID": "blogs/statistics/04-stats.html#background",
    "href": "blogs/statistics/04-stats.html#background",
    "title": "Collider bias",
    "section": "Background",
    "text": "Background\nWhen an exposure and an outcome independently cause a third variable, that variable is termed a ‘collider’. Inappropriately controlling for a collider variable, by study design or statistical analysis, results in collider bias. Controlling for a collider can induce a distorted association between the exposure and outcome, when in fact none exists. This bias predominantly occurs in observational studies. Because collider bias can be induced by sampling, selection bias can sometimes be considered to be a form of collider bias. The diagram below contrasts bias through confounding and collider bias."
  },
  {
    "objectID": "blogs/statistics/04-stats.html#example",
    "href": "blogs/statistics/04-stats.html#example",
    "title": "Collider bias",
    "section": "Example",
    "text": "Example\nA clear example of collider bias was provided by Sackett in his 1979 paper. He analysed data from 257 hospitalized individuals and detected an association between locomotor disease and respiratory disease (odds ratio 4.06). The association seemed plausible at the time – locomotor disease could lead to inactivity, which could cause respiratory disease. But Sackett repeated the analysis in a sample of 2783 individuals from the general population and found no association (odds ratio 1.06). The original analysis of hospitalized individuals was biased because both diseases caused individuals to be hospitalized. By looking only within the stratum of hospitalized individuals, Sackett had observed a distorted association. In contrast, in the general population (including a mix of hospitalized and non-hospitalized individuals) locomotor disease and respiratory disease are not associated. In 1979, Sackett termed this phenomenon “admission rate bias”. With the help of causal diagrams (also known as directed acyclic graphs [DAGs]), this phenomenon can be explained by collider bias (Figure 1).\nIn this example, locomotor disease and respiratory disease are independent causes of hospitalization – the collider (since the two arrowheads collide into hospitalization). If the collider is controlled for by study design (selection bias), a distorted association will arise between locomotor and respiratory disease. This is what we see in Sackett’s 1979 example. Hypothetically, if he had statistically controlled for hospitalization in the general population dataset, he would have induced collider bias again, not through selection, but statistical error.\n\nFigure 1. A causal diagram demonstrating collider bias. Controlling for hospitalization induces a distorted association between locomotor disease and respiratory disease.\nA more recent example of the collider bias can be seen in the ‘obesity paradox’ (Figure 2). This paradox describes an apparent preventive effect of obesity on mortality in individuals with chronic conditions such as cardiovascular disease (CVD). In fact, obesity increases mortality rates in the general population. The collider bias occurs when an investigator conditions on CVD (by design or analysis), resulting in a distorted association between obesity and unmeasured other factors. This distorted association is what distorts the effect of obesity on mortality. Consequently, in a sample that includes only patients with CVD, obesity falsely appears to protect against mortality, whereas in the wider population (with and without CVD), obesity increases the risk of early death. There is some debate about whether collider bias completely explains the obesity paradox.\n\nFigure 2. A causal diagram demonstrating how the obesity paradox can be explained by collider bias."
  },
  {
    "objectID": "blogs/statistics/04-stats.html#impact",
    "href": "blogs/statistics/04-stats.html#impact",
    "title": "Collider bias",
    "section": "Impact",
    "text": "Impact\nCollider bias can have major effects. In Sackett’s example, collider bias inflated a null effect (unbiased odds ratio 1.06) to a positive effect (biased odds ratio 4.06). In the obesity paradox example, collider bias switched an unbiased harmful effect of obesity on mortality into a biased protective effect. This was shown in an analysis of the third US National Health and Nutrition Examination Survey (NHANES III). In the unbiased analysis, the mortality risk ratio for the entire cohort was 1.24 [95% CI = 1.11, 1.39] (harmful). In the biased analysis, the stratum-specific mortality risk ratio was 0.79 [95% CI = 0.68, 0.91] (protective) in patients with CVD.\nThe impact of collider bias – published examples\n\n\n\n\n\n\n\n\nExample\nUnbiased (collider uncontrolled)\nBiased (controlled for collider)\n\n\n\n\nSackett 1979\nOdds ratio 1.06 Null effect\nOdds ratio 4.06 Positive effect\n\n\nObesity paradox\nRisk ratio 1.24 Harmful effect\nRisk ratio 0.79 Protective effect"
  },
  {
    "objectID": "blogs/statistics/04-stats.html#preventive-steps",
    "href": "blogs/statistics/04-stats.html#preventive-steps",
    "title": "Collider bias",
    "section": "Preventive steps",
    "text": "Preventive steps\nCollider bias can be prevented by carefully applying appropriate inclusion criteria – making sure that the exposure and outcome of interest do not drive inclusion or selective retention in a study.\nCausal diagrams (DAGs) can help identify colliders and non-colliders (or confounders). By using these techniques in the design and analysis of observational studies, researchers can identify colliders that should be left uncontrolled and confounders that should be controlled."
  },
  {
    "objectID": "blogs/statistics/06-stats.html",
    "href": "blogs/statistics/06-stats.html",
    "title": "Techniques to adjust for confounding in observational studies",
    "section": "",
    "text": "Sources\nObservational data are often used for research in critical care. Unlike randomized controlled trials, where randomization theoretically balances confounding factors, studies involving observational data pose the challenge of how to adjust appropriately for the bias and confounding that are inherent when comparing two or more groups of patients.\nTable Techniques to adjust for confounding in observational studies"
  },
  {
    "objectID": "blogs/statistics/06-stats.html#matching",
    "href": "blogs/statistics/06-stats.html#matching",
    "title": "Techniques to adjust for confounding in observational studies",
    "section": "Matching",
    "text": "Matching\nA relatively simple technique (although not so commonly used in critical care research) is matching to account for bias and confounding. This involves identifying the variables that constitute confounders and then matching patients in the exposed and unexposed groups based on these variables so that the two groups are “the same” with regard to all of these factors. This is an artificial way of forcing a balance of confounding factors, which would otherwise have been taken care of by randomization. Patients can be matched on as simple a confounder as age or on many individual factors (ventilated on admission to ICU, receiving antibiotics, etc)."
  },
  {
    "objectID": "blogs/statistics/06-stats.html#stratification",
    "href": "blogs/statistics/06-stats.html#stratification",
    "title": "Techniques to adjust for confounding in observational studies",
    "section": "Stratification",
    "text": "Stratification\nThe concept of stratification is similar to matching and provides another simple approach to controlling for confounding. Confounding variables are identified and subgroups created using these variables. For example, if age is a confounder, patients may be grouped into categories of age less than 60, age 60 to 80, and age greater than 80."
  },
  {
    "objectID": "blogs/statistics/06-stats.html#multivariable-adjustment",
    "href": "blogs/statistics/06-stats.html#multivariable-adjustment",
    "title": "Techniques to adjust for confounding in observational studies",
    "section": "Multivariable adjustment",
    "text": "Multivariable adjustment\nThe most common method used for adjustment of confounding in critical care research is multivariable adjustment. After initially examining the relationship between the exposure of interest and the outcome (giving a “crude” or “unadjusted” result), variables that are known confounders are then added to the model to provide an effect that is “adjusted” for these known confounders.\nThe exact model used depends on the research question being asked and specifically whether the outcome variable of interest is binary (eg, hospital mortality), continuous (eg, number of blood transfusions), or involves an outcome associated with time (eg, survival time). If the outcome of interest is binary, then logistic regression modeling is most commonly used."
  },
  {
    "objectID": "blogs/statistics/06-stats.html#propensity-scores",
    "href": "blogs/statistics/06-stats.html#propensity-scores",
    "title": "Techniques to adjust for confounding in observational studies",
    "section": "Propensity Scores",
    "text": "Propensity Scores\nPropensity scores are similar to severity of illness scores, as they provide a single number to represent a large quantity of variables. Rather than estimating severity of illness and probability of hospital mortality, a propensity score is constructed to represent the probability of a patient encountering the exposure of interest by taking into account many variables."
  },
  {
    "objectID": "blogs/statistics/06-stats.html#instrumental-variables",
    "href": "blogs/statistics/06-stats.html#instrumental-variables",
    "title": "Techniques to adjust for confounding in observational studies",
    "section": "Instrumental Variables",
    "text": "Instrumental Variables"
  },
  {
    "objectID": "blogs/tools/02-tools.html",
    "href": "blogs/tools/02-tools.html",
    "title": "Running Jupyter Notebook on a remote server",
    "section": "",
    "text": "Launch Jupyter Notebook from remote server, selecting a port number for :\n# Replace &lt;PORT&gt; with your selected port number\njupyter notebook --no-browser --port=&lt;PORT&gt;\nYou can access the notebook from your remote machine over SSH by setting up a SSH tunnel. Run the following command from your local machine:\n# Replace &lt;PORT&gt; with the port number you selected in the above step\n# Replace &lt;REMOTE_USER&gt; with the remote server username\n# Replace &lt;REMOTE_HOST&gt; with your remote server address\nssh -L 8080:localhost:&lt;PORT&gt; &lt;REMOTE_USER&gt;@&lt;REMOTE_HOST&gt;\nOpen a browser from your local machine and navigate to http://localhost:8080/, the Jupyter Notebook web interface. Replace 8080 with your port number used in step 1.\nNote: if using other port for ssh (such as: (22-&gt;2222)), you should specific the port in the command line:\nssh -L 8080:localhost:8080 pingjian@localhost -p 2222\nsource"
  },
  {
    "objectID": "blogs/tools/02-tools.html#running-jupyter-notebook-on-a-remote-server",
    "href": "blogs/tools/02-tools.html#running-jupyter-notebook-on-a-remote-server",
    "title": "Running Jupyter Notebook on a remote server",
    "section": "",
    "text": "Launch Jupyter Notebook from remote server, selecting a port number for :\n# Replace &lt;PORT&gt; with your selected port number\njupyter notebook --no-browser --port=&lt;PORT&gt;\nYou can access the notebook from your remote machine over SSH by setting up a SSH tunnel. Run the following command from your local machine:\n# Replace &lt;PORT&gt; with the port number you selected in the above step\n# Replace &lt;REMOTE_USER&gt; with the remote server username\n# Replace &lt;REMOTE_HOST&gt; with your remote server address\nssh -L 8080:localhost:&lt;PORT&gt; &lt;REMOTE_USER&gt;@&lt;REMOTE_HOST&gt;\nOpen a browser from your local machine and navigate to http://localhost:8080/, the Jupyter Notebook web interface. Replace 8080 with your port number used in step 1.\nNote: if using other port for ssh (such as: (22-&gt;2222)), you should specific the port in the command line:\nssh -L 8080:localhost:8080 pingjian@localhost -p 2222\nsource"
  },
  {
    "objectID": "blogs/tools/04-tools.html",
    "href": "blogs/tools/04-tools.html",
    "title": "Installing Anaconda on Ubuntu in 3 Steps",
    "section": "",
    "text": "The easiest way to install Anaconda on Ubuntu 18.04, 20.04, and 22.04 is to use the latest Anaconda installation script. In our case, the current version of Anaconda is 2022.05 as of May 10, 2022. The developers might have released a new version by the time you’re reading this, but the installation process should remain the same.\n\nStep 1 – Download and Install Anaconda Script Once you’re logged into your VPS, refresh the APT command to synchronize all repositories via the command line:\n\nsudo apt-get update \ncd /tmp \napt-get install wget \nwget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh \nsha256sum Anaconda3-2022.05-Linux-x86_64.sh \nbash Anaconda3-2022.05-Linux-x86_64.sh\n\nStep 2 – Choose Installation Directory\n\nThe default location is the user’s HOME directory on Ubuntu.\nIt is recommended to have Anaconda installed in this location. Therefore, press Enter to confirm the default location.\n\nStep 3 – Test the Connection\n\nsource ~/.bashrc \nconda info\nSource"
  },
  {
    "objectID": "blogs/tools/06-tools.html",
    "href": "blogs/tools/06-tools.html",
    "title": "Connect Windows With VirtualBox (Ubuntu) using SSH",
    "section": "",
    "text": "准备工作\n\nsudo apt install net-tools\n\n建立IP映射 设置-&gt;网络-&gt;Advanced-&gt;端口转发\n\n名称：ssh\n协议：TCP\n主机IP：127.0.0.1\n主机端口：2222\n子系统端口：22\n\n配置虚拟机SSH\n\n安装openssh-client: sudo apt-get install openssh-client\n安装openssh-server: sudo apt-get install openssh-server\n启动ssh-server: sudo /etc/init.d/ssh restart\n确认ssh-server工作正常：netstat-tpl (看到ssh表示工作正常)\n\n配置虚拟机防火墙\n\n防火墙的一系列操作需要root权限，默认是没有root密码的，所以首先需要设置root密码：\nsudo passwd root\n接着会提示你更新root密码，操作完成以后切换到root账户：\nsu root\n启用2222端口并重启防火墙：\nfirewall-cmd --permanent --add-port=2222/tcp\nfirewall-cmd --reload\n或者直接关闭防火墙：\nsystemctl stop firewalld.service\n\n\nRecommendations:\n\nPuTTY is an SSH and telnet client for Windows.\nWinSCP is a popular SFTP client and FTP client for Windows."
  },
  {
    "objectID": "blogs/tools/08-tools.html",
    "href": "blogs/tools/08-tools.html",
    "title": "Basic Docker Commands",
    "section": "",
    "text": "Source\nSource"
  },
  {
    "objectID": "blogs/tools/08-tools.html#some-basic-docker-commands",
    "href": "blogs/tools/08-tools.html#some-basic-docker-commands",
    "title": "Basic Docker Commands",
    "section": "Some Basic Docker Commands",
    "text": "Some Basic Docker Commands\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\ndocker ps [OPTIONS]\ndocker ps list running containers. docker ps -a list all container including stopped container"
  },
  {
    "objectID": "blogs/tools/10-tools.html",
    "href": "blogs/tools/10-tools.html",
    "title": "How to Plot Figures from My Previous Articles (Python)",
    "section": "",
    "text": "plot_fig\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n    \n    \n    \n    \n\n\n\n\n\n\nIn [15]:\n\n     \nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom data import *\n\ndef get_text(pv):\n    p_text = ''\n    if pv&lt;0.001:\n        p_text = '***'\n    elif 0.001 &lt; pv &lt; 0.01:\n        p_text = '**'\n    elif 0.01 &lt; pv &lt; 0.1:\n        p_text = '*'\n    else:\n        p_text = '#'\n    return p_text\n\ndef plot_ablation(G):\n    plt.figure(figsize=(15,10))\n    \n    labels = [\"TuSDC-S\", \"TuSDC-SC1\", \"TuSDC-SC2\", \"TuSDC-SM\", \"TuSDC\", ]\n\n    ax1 = plt.subplot(2, 3, 1)\n    data = [G.TuSDC_S[0], G.TuSDC_SC1[0], G.TuSDC_SC2[0], G.TuSDC_SM[0], G.TuSDC[0], ]\n    bplot1 = ax1.boxplot(data, notch=True, vert=True, patch_artist=True)\n    ax1.set_ylabel('Hits@10')\n    ax1.set_xticklabels(labels, rotation=30)\n    pv1=stats.ttest_ind(G.TuSDC_SC1[0], G.TuSDC_S[0], alternative='two-sided').pvalue\n    pv2 = stats.ttest_ind(G.TuSDC_SC2[0], G.TuSDC_SC1[0], alternative='two-sided').pvalue\n    pv3 = stats.ttest_ind(G.TuSDC_SM[0], G.TuSDC_S[0], alternative='two-sided').pvalue\n    pv4 = stats.ttest_ind(G.TuSDC[0], G.TuSDC_SC2[0], alternative='two-sided').pvalue\n    pv5 = stats.ttest_ind(G.TuSDC[0], G.TuSDC_SM[0], alternative='two-sided').pvalue\n\n    y = np.array(data).max()\n    h = (max(G.TuSDC_S[0]) - min(G.TuSDC_S[0]))*0.05\n    ax1.plot([1,1,2,2], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv1)\n    ax1.text(1.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax1.plot([2,2,3,3], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv2)\n    ax1.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax1.plot([1,1,4,4], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv3)\n    ax1.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax1.plot([4, 4, 5, 5], [y, y + h, y + h, y], lw=1, color='black')\n    p_text = get_text(pv5)\n    ax1.text(4.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax1.plot([3,3,5,5], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv4)\n    ax1.text(4, y, p_text, ha='center', va='bottom')\n\n    #ax1.text(x=4, y=min(G.TuSDC_S[0]), s='P1=%.2e\\nP2=%.2e\\nP3=%.2e\\nP4=%.2e\\nP5=%.2e'%(pv1,pv2,pv3,pv4,pv5))\n\n    ax2 = plt.subplot(2, 3, 2)\n    data = [G.TuSDC_S[1], G.TuSDC_SC1[1], G.TuSDC_SC2[1], G.TuSDC_SM[1], G.TuSDC[1], ]\n    bplot2 = ax2.boxplot(data, notch=True, vert=True, patch_artist=True)\n    ax2.set_ylabel('Hits@3')\n    ax2.set_xticklabels(labels, rotation=30)\n    pv1=stats.ttest_ind(G.TuSDC_SC1[1], G.TuSDC_S[1], alternative='two-sided').pvalue\n    pv2 = stats.ttest_ind(G.TuSDC_SC2[1], G.TuSDC_SC1[1], alternative='two-sided').pvalue\n    pv3 = stats.ttest_ind(G.TuSDC_SM[1], G.TuSDC_S[1], alternative='two-sided').pvalue\n    pv4 = stats.ttest_ind(G.TuSDC[1], G.TuSDC_SC2[1], alternative='two-sided').pvalue\n    pv5 = stats.ttest_ind(G.TuSDC[1], G.TuSDC_SM[1], alternative='two-sided').pvalue\n\n    y = np.array(data).max()\n    h = (max(G.TuSDC_S[1])-min(G.TuSDC_S[1]))*0.05\n    ax2.plot([1,1,2,2], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv1)\n    ax2.text(1.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax2.plot([2,2,3,3], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv2)\n    ax2.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax2.plot([1,1,4,4], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv3)\n    ax2.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax2.plot([4, 4, 5, 5], [y, y + h, y + h, y], lw=1, color='black')\n    p_text = get_text(pv5)\n    ax2.text(4.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax2.plot([3,3,5,5], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv4)\n    ax2.text(4, y, p_text, ha='center', va='bottom')\n\n    ax3 = plt.subplot(2, 3, 3)\n    data = [G.TuSDC_S[2], G.TuSDC_SC1[2], G.TuSDC_SC2[2], G.TuSDC_SM[2], G.TuSDC[2], ]\n    bplot3 = ax3.boxplot(data, notch=True, vert=True, patch_artist=True)\n    ax3.set_ylabel('Hits@1')\n    ax3.set_xticklabels(labels, rotation=30)\n    pv1=stats.ttest_ind(G.TuSDC_SC1[2], G.TuSDC_S[2], alternative='two-sided').pvalue\n    pv2 = stats.ttest_ind(G.TuSDC_SC2[2], G.TuSDC_SC1[2], alternative='two-sided').pvalue\n    pv3 = stats.ttest_ind(G.TuSDC_SM[2], G.TuSDC_S[2], alternative='two-sided').pvalue\n    pv4 = stats.ttest_ind(G.TuSDC[2], G.TuSDC_SC2[2], alternative='two-sided').pvalue\n    pv5 = stats.ttest_ind(G.TuSDC[2], G.TuSDC_SM[2], alternative='two-sided').pvalue\n\n    y = np.array(data).max()\n    h = (max(G.TuSDC_S[2]) - min(G.TuSDC_S[2]))*0.05\n    ax3.plot([1,1,2,2], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv1)\n    ax3.text(1.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax3.plot([2,2,3,3], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv2)\n    ax3.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax3.plot([1,1,4,4], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv3)\n    ax3.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax3.plot([4, 4, 5, 5], [y, y + h, y + h, y], lw=1, color='black')\n    p_text = get_text(pv5)\n    ax3.text(4.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax3.plot([3,3,5,5], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv4)\n    ax3.text(4, y, p_text, ha='center', va='bottom')\n\n    ax4 = plt.subplot(2, 3, 4)\n    data = [G.TuSDC_S[4], G.TuSDC_SC1[4], G.TuSDC_SC2[4], G.TuSDC_SM[4], G.TuSDC[4], ]\n    bplot4 = ax4.boxplot(data, notch=True, vert=True, patch_artist=True)\n    ax4.set_ylabel('MRR')\n    ax4.set_xticklabels(labels, rotation=30)\n    pv1=stats.ttest_ind(G.TuSDC_SC1[4], G.TuSDC_S[4], alternative='two-sided').pvalue\n    pv2 = stats.ttest_ind(G.TuSDC_SC2[4], G.TuSDC_SC1[4], alternative='two-sided').pvalue\n    pv3 = stats.ttest_ind(G.TuSDC_SM[4], G.TuSDC_S[4], alternative='two-sided').pvalue\n    pv4 = stats.ttest_ind(G.TuSDC[4], G.TuSDC_SC2[4], alternative='two-sided').pvalue\n    pv5 = stats.ttest_ind(G.TuSDC[4], G.TuSDC_SM[4], alternative='two-sided').pvalue\n\n    y = np.array(data).max()\n    h = (max(G.TuSDC_S[4])-min(G.TuSDC_S[4]))*0.05\n    ax4.plot([1,1,2,2], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv1)\n    ax4.text(1.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax4.plot([2,2,3,3], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv2)\n    ax4.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax4.plot([1,1,4,4], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv3)\n    ax4.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax4.plot([4, 4, 5, 5], [y, y + h, y + h, y], lw=1, color='black')\n    p_text = get_text(pv5)\n    ax4.text(4.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax4.plot([3,3,5,5], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv4)\n    ax4.text(4, y, p_text, ha='center', va='bottom')\n\n    ax5 = plt.subplot(2, 3, 5)\n    data = [G.TuSDC_S[5], G.TuSDC_SC1[5], G.TuSDC_SC2[5], G.TuSDC_SM[5], G.TuSDC[5], ]\n    bplot5 = ax5.boxplot(data, notch=True, vert=True, patch_artist=True)\n    ax5.set_ylabel('AUPR')\n    ax5.set_xticklabels(labels, rotation=30)\n    pv1=stats.ttest_ind(G.TuSDC_SC1[5], G.TuSDC_S[5], alternative='two-sided').pvalue\n    pv2 = stats.ttest_ind(G.TuSDC_SC2[5], G.TuSDC_SC1[5], alternative='two-sided').pvalue\n    pv3 = stats.ttest_ind(G.TuSDC_SM[5], G.TuSDC_S[5], alternative='two-sided').pvalue\n    pv4 = stats.ttest_ind(G.TuSDC[5], G.TuSDC_SC2[5], alternative='two-sided').pvalue\n    pv5 = stats.ttest_ind(G.TuSDC[5], G.TuSDC_SM[5], alternative='two-sided').pvalue\n    #ax5.text(x=4, y=min(G.TuSDC_S[5]), s='P1=%.2e\\nP2=%.2e\\nP3=%.2e\\nP4=%.2e\\nP5=%.2e'%(pv1,pv2,pv3,pv4,pv5))\n\n    y = np.array(data).max()\n    h = (max(G.TuSDC_S[5])-min(G.TuSDC_S[5]))*0.05\n    ax5.plot([1,1,2,2], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv1)\n    ax5.text(1.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax5.plot([2,2,3,3], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv2)\n    ax5.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax5.plot([1,1,4,4], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv3)\n    ax5.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax5.plot([4, 4, 5, 5], [y, y + h, y + h, y], lw=1, color='black')\n    p_text = get_text(pv5)\n    ax5.text(4.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax5.plot([3,3,5,5], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv4)\n    ax5.text(4, y, p_text, ha='center', va='bottom')\n\n    ax6 = plt.subplot(2, 3, 6)\n    data = [G.TuSDC_S[6], G.TuSDC_SC1[6], G.TuSDC_SC2[6], G.TuSDC_SM[6], G.TuSDC[6], ]\n    bplot6 = ax6.boxplot(data, notch=True, vert=True, patch_artist=True)\n    ax6.set_ylabel('AUC')\n    ax6.set_xticklabels(labels, rotation=30)\n    pv1=stats.ttest_ind(G.TuSDC_SC1[6], G.TuSDC_S[6], alternative='two-sided').pvalue\n    pv2 = stats.ttest_ind(G.TuSDC_SC2[6], G.TuSDC_SC1[6], alternative='two-sided').pvalue\n    pv3 = stats.ttest_ind(G.TuSDC_SM[6], G.TuSDC_S[6], alternative='two-sided').pvalue\n    pv4 = stats.ttest_ind(G.TuSDC[6], G.TuSDC_SC2[6], alternative='two-sided').pvalue\n    pv5 = stats.ttest_ind(G.TuSDC[6], G.TuSDC_SM[6], alternative='two-sided').pvalue\n\n    y = np.array(data).max()\n    h = (max(G.TuSDC_S[6]) - min(G.TuSDC_S[6]))*0.05\n    ax6.plot([1,1,2,2], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv1)\n    ax6.text(1.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax6.plot([2,2,3,3], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv2)\n    ax6.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax6.plot([1,1,4,4], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv3)\n    ax6.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax6.plot([4, 4, 5, 5], [y, y + h, y + h, y], lw=1, color='black')\n    p_text = get_text(pv5)\n    ax6.text(4.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax6.plot([3,3,5,5], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv4)\n    ax6.text(4, y, p_text, ha='center', va='bottom')\n\n    colors = ['pink', 'lightblue', 'lightgreen', 'lightyellow', 'lightgray']\n    for bplot in (bplot1, bplot2, bplot3, bplot4, bplot5, bplot6):\n        for patch, color in zip(bplot['boxes'], colors):\n            patch.set_facecolor(color)\n    return plt\n\nG = ablation_data('ablation_global') # 'ablation_global' or 'ablation_local'\nplt = plot_ablation(G)\nplt.show()"
  },
  {
    "objectID": "blogs/tools/10-tools.html#box-plot",
    "href": "blogs/tools/10-tools.html#box-plot",
    "title": "How to Plot Figures from My Previous Articles (Python)",
    "section": "",
    "text": "plot_fig\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n    \n    \n    \n    \n\n\n\n\n\n\nIn [15]:\n\n     \nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom data import *\n\ndef get_text(pv):\n    p_text = ''\n    if pv&lt;0.001:\n        p_text = '***'\n    elif 0.001 &lt; pv &lt; 0.01:\n        p_text = '**'\n    elif 0.01 &lt; pv &lt; 0.1:\n        p_text = '*'\n    else:\n        p_text = '#'\n    return p_text\n\ndef plot_ablation(G):\n    plt.figure(figsize=(15,10))\n    \n    labels = [\"TuSDC-S\", \"TuSDC-SC1\", \"TuSDC-SC2\", \"TuSDC-SM\", \"TuSDC\", ]\n\n    ax1 = plt.subplot(2, 3, 1)\n    data = [G.TuSDC_S[0], G.TuSDC_SC1[0], G.TuSDC_SC2[0], G.TuSDC_SM[0], G.TuSDC[0], ]\n    bplot1 = ax1.boxplot(data, notch=True, vert=True, patch_artist=True)\n    ax1.set_ylabel('Hits@10')\n    ax1.set_xticklabels(labels, rotation=30)\n    pv1=stats.ttest_ind(G.TuSDC_SC1[0], G.TuSDC_S[0], alternative='two-sided').pvalue\n    pv2 = stats.ttest_ind(G.TuSDC_SC2[0], G.TuSDC_SC1[0], alternative='two-sided').pvalue\n    pv3 = stats.ttest_ind(G.TuSDC_SM[0], G.TuSDC_S[0], alternative='two-sided').pvalue\n    pv4 = stats.ttest_ind(G.TuSDC[0], G.TuSDC_SC2[0], alternative='two-sided').pvalue\n    pv5 = stats.ttest_ind(G.TuSDC[0], G.TuSDC_SM[0], alternative='two-sided').pvalue\n\n    y = np.array(data).max()\n    h = (max(G.TuSDC_S[0]) - min(G.TuSDC_S[0]))*0.05\n    ax1.plot([1,1,2,2], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv1)\n    ax1.text(1.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax1.plot([2,2,3,3], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv2)\n    ax1.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax1.plot([1,1,4,4], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv3)\n    ax1.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax1.plot([4, 4, 5, 5], [y, y + h, y + h, y], lw=1, color='black')\n    p_text = get_text(pv5)\n    ax1.text(4.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax1.plot([3,3,5,5], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv4)\n    ax1.text(4, y, p_text, ha='center', va='bottom')\n\n    #ax1.text(x=4, y=min(G.TuSDC_S[0]), s='P1=%.2e\\nP2=%.2e\\nP3=%.2e\\nP4=%.2e\\nP5=%.2e'%(pv1,pv2,pv3,pv4,pv5))\n\n    ax2 = plt.subplot(2, 3, 2)\n    data = [G.TuSDC_S[1], G.TuSDC_SC1[1], G.TuSDC_SC2[1], G.TuSDC_SM[1], G.TuSDC[1], ]\n    bplot2 = ax2.boxplot(data, notch=True, vert=True, patch_artist=True)\n    ax2.set_ylabel('Hits@3')\n    ax2.set_xticklabels(labels, rotation=30)\n    pv1=stats.ttest_ind(G.TuSDC_SC1[1], G.TuSDC_S[1], alternative='two-sided').pvalue\n    pv2 = stats.ttest_ind(G.TuSDC_SC2[1], G.TuSDC_SC1[1], alternative='two-sided').pvalue\n    pv3 = stats.ttest_ind(G.TuSDC_SM[1], G.TuSDC_S[1], alternative='two-sided').pvalue\n    pv4 = stats.ttest_ind(G.TuSDC[1], G.TuSDC_SC2[1], alternative='two-sided').pvalue\n    pv5 = stats.ttest_ind(G.TuSDC[1], G.TuSDC_SM[1], alternative='two-sided').pvalue\n\n    y = np.array(data).max()\n    h = (max(G.TuSDC_S[1])-min(G.TuSDC_S[1]))*0.05\n    ax2.plot([1,1,2,2], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv1)\n    ax2.text(1.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax2.plot([2,2,3,3], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv2)\n    ax2.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax2.plot([1,1,4,4], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv3)\n    ax2.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax2.plot([4, 4, 5, 5], [y, y + h, y + h, y], lw=1, color='black')\n    p_text = get_text(pv5)\n    ax2.text(4.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax2.plot([3,3,5,5], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv4)\n    ax2.text(4, y, p_text, ha='center', va='bottom')\n\n    ax3 = plt.subplot(2, 3, 3)\n    data = [G.TuSDC_S[2], G.TuSDC_SC1[2], G.TuSDC_SC2[2], G.TuSDC_SM[2], G.TuSDC[2], ]\n    bplot3 = ax3.boxplot(data, notch=True, vert=True, patch_artist=True)\n    ax3.set_ylabel('Hits@1')\n    ax3.set_xticklabels(labels, rotation=30)\n    pv1=stats.ttest_ind(G.TuSDC_SC1[2], G.TuSDC_S[2], alternative='two-sided').pvalue\n    pv2 = stats.ttest_ind(G.TuSDC_SC2[2], G.TuSDC_SC1[2], alternative='two-sided').pvalue\n    pv3 = stats.ttest_ind(G.TuSDC_SM[2], G.TuSDC_S[2], alternative='two-sided').pvalue\n    pv4 = stats.ttest_ind(G.TuSDC[2], G.TuSDC_SC2[2], alternative='two-sided').pvalue\n    pv5 = stats.ttest_ind(G.TuSDC[2], G.TuSDC_SM[2], alternative='two-sided').pvalue\n\n    y = np.array(data).max()\n    h = (max(G.TuSDC_S[2]) - min(G.TuSDC_S[2]))*0.05\n    ax3.plot([1,1,2,2], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv1)\n    ax3.text(1.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax3.plot([2,2,3,3], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv2)\n    ax3.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax3.plot([1,1,4,4], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv3)\n    ax3.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax3.plot([4, 4, 5, 5], [y, y + h, y + h, y], lw=1, color='black')\n    p_text = get_text(pv5)\n    ax3.text(4.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax3.plot([3,3,5,5], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv4)\n    ax3.text(4, y, p_text, ha='center', va='bottom')\n\n    ax4 = plt.subplot(2, 3, 4)\n    data = [G.TuSDC_S[4], G.TuSDC_SC1[4], G.TuSDC_SC2[4], G.TuSDC_SM[4], G.TuSDC[4], ]\n    bplot4 = ax4.boxplot(data, notch=True, vert=True, patch_artist=True)\n    ax4.set_ylabel('MRR')\n    ax4.set_xticklabels(labels, rotation=30)\n    pv1=stats.ttest_ind(G.TuSDC_SC1[4], G.TuSDC_S[4], alternative='two-sided').pvalue\n    pv2 = stats.ttest_ind(G.TuSDC_SC2[4], G.TuSDC_SC1[4], alternative='two-sided').pvalue\n    pv3 = stats.ttest_ind(G.TuSDC_SM[4], G.TuSDC_S[4], alternative='two-sided').pvalue\n    pv4 = stats.ttest_ind(G.TuSDC[4], G.TuSDC_SC2[4], alternative='two-sided').pvalue\n    pv5 = stats.ttest_ind(G.TuSDC[4], G.TuSDC_SM[4], alternative='two-sided').pvalue\n\n    y = np.array(data).max()\n    h = (max(G.TuSDC_S[4])-min(G.TuSDC_S[4]))*0.05\n    ax4.plot([1,1,2,2], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv1)\n    ax4.text(1.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax4.plot([2,2,3,3], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv2)\n    ax4.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax4.plot([1,1,4,4], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv3)\n    ax4.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax4.plot([4, 4, 5, 5], [y, y + h, y + h, y], lw=1, color='black')\n    p_text = get_text(pv5)\n    ax4.text(4.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax4.plot([3,3,5,5], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv4)\n    ax4.text(4, y, p_text, ha='center', va='bottom')\n\n    ax5 = plt.subplot(2, 3, 5)\n    data = [G.TuSDC_S[5], G.TuSDC_SC1[5], G.TuSDC_SC2[5], G.TuSDC_SM[5], G.TuSDC[5], ]\n    bplot5 = ax5.boxplot(data, notch=True, vert=True, patch_artist=True)\n    ax5.set_ylabel('AUPR')\n    ax5.set_xticklabels(labels, rotation=30)\n    pv1=stats.ttest_ind(G.TuSDC_SC1[5], G.TuSDC_S[5], alternative='two-sided').pvalue\n    pv2 = stats.ttest_ind(G.TuSDC_SC2[5], G.TuSDC_SC1[5], alternative='two-sided').pvalue\n    pv3 = stats.ttest_ind(G.TuSDC_SM[5], G.TuSDC_S[5], alternative='two-sided').pvalue\n    pv4 = stats.ttest_ind(G.TuSDC[5], G.TuSDC_SC2[5], alternative='two-sided').pvalue\n    pv5 = stats.ttest_ind(G.TuSDC[5], G.TuSDC_SM[5], alternative='two-sided').pvalue\n    #ax5.text(x=4, y=min(G.TuSDC_S[5]), s='P1=%.2e\\nP2=%.2e\\nP3=%.2e\\nP4=%.2e\\nP5=%.2e'%(pv1,pv2,pv3,pv4,pv5))\n\n    y = np.array(data).max()\n    h = (max(G.TuSDC_S[5])-min(G.TuSDC_S[5]))*0.05\n    ax5.plot([1,1,2,2], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv1)\n    ax5.text(1.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax5.plot([2,2,3,3], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv2)\n    ax5.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax5.plot([1,1,4,4], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv3)\n    ax5.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax5.plot([4, 4, 5, 5], [y, y + h, y + h, y], lw=1, color='black')\n    p_text = get_text(pv5)\n    ax5.text(4.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax5.plot([3,3,5,5], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv4)\n    ax5.text(4, y, p_text, ha='center', va='bottom')\n\n    ax6 = plt.subplot(2, 3, 6)\n    data = [G.TuSDC_S[6], G.TuSDC_SC1[6], G.TuSDC_SC2[6], G.TuSDC_SM[6], G.TuSDC[6], ]\n    bplot6 = ax6.boxplot(data, notch=True, vert=True, patch_artist=True)\n    ax6.set_ylabel('AUC')\n    ax6.set_xticklabels(labels, rotation=30)\n    pv1=stats.ttest_ind(G.TuSDC_SC1[6], G.TuSDC_S[6], alternative='two-sided').pvalue\n    pv2 = stats.ttest_ind(G.TuSDC_SC2[6], G.TuSDC_SC1[6], alternative='two-sided').pvalue\n    pv3 = stats.ttest_ind(G.TuSDC_SM[6], G.TuSDC_S[6], alternative='two-sided').pvalue\n    pv4 = stats.ttest_ind(G.TuSDC[6], G.TuSDC_SC2[6], alternative='two-sided').pvalue\n    pv5 = stats.ttest_ind(G.TuSDC[6], G.TuSDC_SM[6], alternative='two-sided').pvalue\n\n    y = np.array(data).max()\n    h = (max(G.TuSDC_S[6]) - min(G.TuSDC_S[6]))*0.05\n    ax6.plot([1,1,2,2], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv1)\n    ax6.text(1.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax6.plot([2,2,3,3], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv2)\n    ax6.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2*h\n    ax6.plot([1,1,4,4], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv3)\n    ax6.text(2.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax6.plot([4, 4, 5, 5], [y, y + h, y + h, y], lw=1, color='black')\n    p_text = get_text(pv5)\n    ax6.text(4.5, y, p_text, ha='center', va='bottom')\n    y = y + 2 * h\n    ax6.plot([3,3,5,5], [y,y+h,y+h,y], lw=1, color='black')\n    p_text = get_text(pv4)\n    ax6.text(4, y, p_text, ha='center', va='bottom')\n\n    colors = ['pink', 'lightblue', 'lightgreen', 'lightyellow', 'lightgray']\n    for bplot in (bplot1, bplot2, bplot3, bplot4, bplot5, bplot6):\n        for patch, color in zip(bplot['boxes'], colors):\n            patch.set_facecolor(color)\n    return plt\n\nG = ablation_data('ablation_global') # 'ablation_global' or 'ablation_local'\nplt = plot_ablation(G)\nplt.show()"
  },
  {
    "objectID": "books/bioinformatics.html",
    "href": "books/bioinformatics.html",
    "title": "Introduction to Bioinformatics",
    "section": "",
    "text": "Introduction to Bioinformatics"
  },
  {
    "objectID": "books/epidemiology.html",
    "href": "books/epidemiology.html",
    "title": "Introduction to Probability and Statistics for Epidemiology",
    "section": "",
    "text": "Source\n\n\nThis course aims to provide a firm grounding in the foundations of probability and statistics. Specific topics include:\n\nDescribing data (types of data, data visualization, descriptive statistics)\nStatistical inference (probability, probability distributions, sampling theory, hypothesis testing, confidence intervals, pitfalls of p-values)\nSpecific statistical tests (ttest, ANOVA, linear correlation, non-parametric tests, relative risks, Chi-square test, exact tests, linear regression, logistic regression, survival analysis; how to choose the right statistical test)\n\nThe course focuses on real examples from the medical literature and popular press. Each week starts with “teasers,” such as: Should I be worried about lead in lipstick? Should I play the lottery when the jackpot reaches half-a-billion dollars? Does eating red meat increase my risk of being in a traffic accident? We will work our way back from the news coverage to the original study and then to the underlying data. In the process, students will learn how to read, interpret, and critically evaluate the statistics in medical studies.\nThe course also prepares students to be able to analyze their own data, guiding them on how to choose the correct statistical test and how to avoid common statistical pitfalls. Optional modules cover advanced math topics and basic data analysis in R.\n\n\n\n\nWeek 1 - Descriptive statistics and looking at data\nWeek 2 - Review of study designs; measures of disease risk and association\nWeek 3 - Probability, Bayes’ Rule, Diagnostic Testing\nWeek 4 - Probability distributions\nWeek 5 - Statistical inference (confidence intervals and hypothesis testing)\nWeek 6 - P-value pitfalls; types I and type II error; statistical power; overview of statistical tests\nWeek 7 - Tests for comparing groups (unadjusted); introduction to survival analysis\nWeek 8 - Regression analysis; linear correlation and regression\nWeek 9 - Logistic regression and Cox regression"
  },
  {
    "objectID": "books/epidemiology.html#about-this-course",
    "href": "books/epidemiology.html#about-this-course",
    "title": "Introduction to Probability and Statistics for Epidemiology",
    "section": "",
    "text": "This course aims to provide a firm grounding in the foundations of probability and statistics. Specific topics include:\n\nDescribing data (types of data, data visualization, descriptive statistics)\nStatistical inference (probability, probability distributions, sampling theory, hypothesis testing, confidence intervals, pitfalls of p-values)\nSpecific statistical tests (ttest, ANOVA, linear correlation, non-parametric tests, relative risks, Chi-square test, exact tests, linear regression, logistic regression, survival analysis; how to choose the right statistical test)\n\nThe course focuses on real examples from the medical literature and popular press. Each week starts with “teasers,” such as: Should I be worried about lead in lipstick? Should I play the lottery when the jackpot reaches half-a-billion dollars? Does eating red meat increase my risk of being in a traffic accident? We will work our way back from the news coverage to the original study and then to the underlying data. In the process, students will learn how to read, interpret, and critically evaluate the statistics in medical studies.\nThe course also prepares students to be able to analyze their own data, guiding them on how to choose the correct statistical test and how to avoid common statistical pitfalls. Optional modules cover advanced math topics and basic data analysis in R."
  },
  {
    "objectID": "books/epidemiology.html#course-syllabus",
    "href": "books/epidemiology.html#course-syllabus",
    "title": "Introduction to Probability and Statistics for Epidemiology",
    "section": "",
    "text": "Week 1 - Descriptive statistics and looking at data\nWeek 2 - Review of study designs; measures of disease risk and association\nWeek 3 - Probability, Bayes’ Rule, Diagnostic Testing\nWeek 4 - Probability distributions\nWeek 5 - Statistical inference (confidence intervals and hypothesis testing)\nWeek 6 - P-value pitfalls; types I and type II error; statistical power; overview of statistical tests\nWeek 7 - Tests for comparing groups (unadjusted); introduction to survival analysis\nWeek 8 - Regression analysis; linear correlation and regression\nWeek 9 - Logistic regression and Cox regression"
  },
  {
    "objectID": "books/statistical_learning.html",
    "href": "books/statistical_learning.html",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Materials Materials\nNew techniques have emerged for both predictive and descriptive learning that help us make sense of vast and complex data sets. The particular focus of this course will be on regression and classification methods as tools for facilitating machine learning. This course is in a flipped format: there will be pre-recorded lectures and in-class problem solving and discussion sessions will be used.\nTopics Include\n\nOverview of statistical learning\nLinear regression\nClassification\nResampling methods\nLinear model selection and regularization\nMoving beyond linearity\nTree-based methods\nSupport vector machines\nUnsupervised learning"
  },
  {
    "objectID": "books/statistical_learning.html#about-this-course",
    "href": "books/statistical_learning.html#about-this-course",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Materials Materials\nNew techniques have emerged for both predictive and descriptive learning that help us make sense of vast and complex data sets. The particular focus of this course will be on regression and classification methods as tools for facilitating machine learning. This course is in a flipped format: there will be pre-recorded lectures and in-class problem solving and discussion sessions will be used.\nTopics Include\n\nOverview of statistical learning\nLinear regression\nClassification\nResampling methods\nLinear model selection and regularization\nMoving beyond linearity\nTree-based methods\nSupport vector machines\nUnsupervised learning"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "Introduction to Probability and Statistics for Epidemiology\nIntroduction to Statistical Learning\nIntroduction to Bioinformatics\nData Analysis and Visualisation\nGenome-wide Association Studies\nNumerical Opitimization\nIntroduction to Deep Learning"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "2024\n\nZhenxiang Gao#, Pingjian Ding#, and Rong Xu*. “IUPHAR review-Data-driven Computational Drug Repurposing Approaches for Opioid Use Disorder.” Pharmacological Research 199 (2024): 106960. [Link]\nHanyu Luo, Li Tang, Min Zeng, Rui Yin, Pingjian Ding, Lingyun Luo, and Min Li. “BertSNR: an interpretable deep learning framework for single nucleotide resolution identification of transcription factor binding sites based on DNA language model.” Bioinformatics (2024). [Link]\nAnlin Hou, Hanyu Luo, Huan Liu, Lingyun Luo, and Pingjian Ding. “Multi-scale DNA language model improves 6 mA binding sites prediction.” Computational Biology and Chemistry 112(17):108129. (2024). [Link]\nZhenyu Jiang, Pingjian Ding, Cong Shen, and Xiaopeng Dai. “Geometric molecular graph representation learning model for drug-drug interactions prediction.” (2024). IEEE Journal of Biomedical and Health Informatics. [Link]\nYuxun Luo, Wenyu Shan, Li Peng, Lingyun Luo, Pingjian Ding, and Wei Liang. “A computational framework for predicting novel drug indications using graph convolutional network with contrastive learning.” IEEE Journal of Biomedical and Health Informatics (2024). [Link]\nZiyu Wu, Shasha Li, Lingyun Luo, and Pingjian Ding*. “HKFGCN: A novel multiple kernel fusion framework on graph convolutional network to predict microbe-drug associations.” Computational Biology and Chemistry (2024): 108041. [Link]\nYuxun Luo, Shasha Li, Li Peng, Pingjian Ding, and Wei Liang. “Predicting associations between drugs and G Protein-Coupled Receptors using a multi-graph convolutional network.” Computational Biology and Chemistry (2024): 108060. [Link]\nCong Shen, Pingjian Ding, Junjie Wee, Jialin Bi, Jiawei Luo, and Kelin Xiaa. “Curvature-enhanced graph convolutional network for biomolecular interaction prediction.” Computational and Structural Biotechnology Journal (2024). [Link]\nMin Li, Baoying Zhao, Yiming Li, Pingjian Ding, Rui Yin, Shichao Kan, Yi Pan, and Min Zeng. “SGCL-LncLoc: an interpretable deep learning model for improving lncRNA subcellular localization prediction with supervised graph contrastive learning.” Big Data Mining and Analytics (2024). [Link]\nJiang, Zhenyu, Zhi Gong, Xiaopeng Dai, Hongyan Zhang, Pingjian Ding, and Cong Shen. “Deep graph contrastive learning model for drug-drug interaction prediction.” PloS one 19, no. 6 (2024): e0304798. [Link]\n\n\n\n\n2023\n\nPan Zheng*, Xiangxiang Zeng, Xun Wang, Pingjian Ding. “Editorial: Artificial intelligence and machine learning for drug discovery, design and repurposing: methods and applications.” Frontiers in Pharmacology (2023). [Link]\nPingjian Ding, and Rong Xu*. “Causal association of COVID-19 with brain structure changes: Findings from a non-overlapping 2-sample Mendelian randomization study.” Journal of the Neurological Sciences (2023): 2023-07. [Link]\nPingjian Ding, Mark Gurney, George Perry, Rong Xu*. “Association of COVID-19 with risk and progression of Alzheimer’s disease: non-overlapping two-sample Mendelian randomization analysis of 2.6 million subjects.” Journal of Alzheimer’s Disease (2023). [Link]\nWenyu Shan, Cong Shen, Lingyun Luo, and Pingjian Ding*. “Multi-task Learning for Predicting Synergistic Drug Combinations based on Auto-Encoding Multi-Relational Graphs.” iScience (2023). [Link]\nYichen Zhong, Xiaoting Xi, Cong Shen, Yuxun Luo, Pingjian Ding, and Lingyun Luo*. “Multitask joint learning with graph autoencoders for predicting potential MiRNA-drug associations.” Artificial Intelligence in Medicine (2023): 102665. [Link]\nCheng Chen, Lingyun Luo*, Chunlei Zheng, Pingjian Ding*, Huan Liu, and Hanyu Luo*. “Self-prediction of relations in GO facilitates its quality auditing.” Journal of Biomedical Informatics 144 (2023): 104441. [Link]\nHanyu Luo, Ye Li, Huan Liu, Pingjian Ding, Ying Yu, and Lingyun Luo*. “SENet: a deep learning framework for discriminating super-and typical enhancers by sequence information.” Computational Biology and Chemistry (2023): 107905. [Link]\nHanyu Luo, Wenyu Shan, Cheng Chen, Pingjian Ding, and Lingyun Luo*. “Improving language model of human genome for DNA–protein binding prediction based on task-specific pre-training.” Interdisciplinary Sciences: Computational Life Sciences 15, no. 1 (2023): 32-43. [Link]\nPingjian Ding*, Min Zeng*, and Rui Yin*. “Editorial: Computational methods to analyze RNA data for human diseases.” Frontiers in Genetics 14 (2023). [Link]\nPingjian Ding, Maria P. Gorenflo, Xiaofeng Zhu, and Rong Xu*. “Aspirin Use and Risk of Alzheimer’s Disease: A 2-Sample Mendelian Randomization Study.” Journal of Alzheimer’s Disease (2023): 1-12. [Link]\n\n\n\n\n2022\n\nPingjian Ding#, Yiheng Pan#, Quanqiu Wang, and Rong Xu*. “Prediction and evaluation of combination pharmacotherapy using natural language processing, machine learning and patient electronic health records.” Journal of Biomedical Informatics 133 (2022): 104164. [Link]\nHanyu Luo, Cheng Chen, Wenyu Shan, Pingjian Ding, and Lingyun Luo*. “iEnhancer-BERT: A novel transfer learning architecture based on DNA-Language model for identifying enhancers and their strength.” In International Conference on Intelligent Computing, pp. 153-165. Cham: Springer International Publishing, 2022. [Link]\nZhenxiang Gao, Pingjian Ding, and Rong Xu*. “KG-Predict: A knowledge graph computational framework for drug repurposing.” Journal of biomedical informatics 132 (2022): 104133. [Link]\nZhenxiang Gao, Yiheng Pan, Pingjian Ding, and Rong Xu*. “A knowledge graph-based disease-gene prediction system using multi-relational graph convolution networks.” In AMIA Annual Symposium Proceedings, vol. 2022, p. 468. American Medical Informatics Association, 2022. [Link]\n\n\n\n\n2021\n\nXin Chen, Lingyun Luo, Cong Shen, Pingjian Ding*, and Jiawei Luo. “An in silico method for predicting drug synergy based on multitask learning.” Interdisciplinary Sciences: Computational Life Sciences 13 (2021): 299-311. [Link]\nPingjian Ding, Cheng Liang, Wenjue Ouyang, Guanghui Li, Qiu Xiao, and Jiawei Luo*. “Inferring synergistic drug combinations based on symmetric meta-path in a novel heterogeneous network.” IEEE/ACM Transactions on Computational Biology and Bioinformatics 18, no. 4 (2019): 1562-1571. [Link]\n\n\n\n\n2020\n\nCong Shen, Jiawei Luo*, Wenjue Ouyang, Pingjian Ding, and Xiangtao Chen. “IDDkin: network-based influence deep diffusion model for enhancing prediction of kinase inhibitors.” Bioinformatics 36, no. 22-23 (2020): 5481-5491. [Link]\nLi, Guanghui, Jiawei Luo*, Diancheng Wang, Cheng Liang, Qiu Xiao, Pingjian Ding, and Hailin Chen. “Potential circRNA-disease association prediction using DeepWalk and network consistency projection.” Journal of biomedical informatics 112 (2020): 103624. [Link]\nCong Shen, Jiawei Luo*, Wenjue Ouyang, Pingjian Ding, and Hao Wu. “Identification of small molecule–miRNA associations with graph regularization techniques in heterogeneous networks.” Journal of Chemical Information and Modeling 60, no. 12 (2020): 6709-6721. [Link]\nQiu Xiao, Haiming Yu, Jiancheng Zhong, Cheng Liang, Guanghui Li, Pingjian Ding, and Jiawei Luo*. “An in-silico method with graph-based multi-label learning for large-scale prediction of circRNA-disease associations.” Genomics 112, no. 5 (2020): 3407-3415. [Link]\nCong Shen, Jiawei Luo, Zihan Lai, and Pingjian Ding. “Multiview joint learning-based method for identifying small-molecule-associated MiRNAs by integrating pharmacological, genomics, and network knowledge.” Journal of Chemical Information and Modeling 60, no. 8 (2020): 4085-4097. [Link]\nPingjian Ding, Wenjue Ouyang, Jiawei Luo*, and Chee-Keong Kwoh. “Heterogeneous information network and its application to human health and disease.” Briefings in bioinformatics 21, no. 4 (2020): 1327-1346. [Link]\nJiawei Luo*, Cong Shen, Zihan Lai, Jie Cai, and Pingjian Ding. “Incorporating clinical, chemical and biological information for predicting small molecule-microRNA associations based on non-negative matrix factorization.” IEEE/ACM Transactions on Computational Biology and Bioinformatics 18, no. 6 (2020): 2535-2545. [Link]\nHaojiang Tan, Quanmeng Sun, Guanghui Li*, Qiu Xiao, Pingjian Ding, Jiawei Luo, and Cheng Liang. “Multiview consensus graph learning for lncRNA–disease association prediction.” Frontiers in genetics 11 (2020): 89. [Link]\nPingjian Ding, Cong Shen, Zihan Lai, Cheng Liang, Guanghui Li, and Jiawei Luo*. “Incorporating multisource knowledge to predict drug synergy based on graph co-regularization.” Journal of Chemical Information and Modeling 60, no. 1 (2019): 37-46. [Link]\n\n\n\n\n2019\n\nZhenxia Pan, Huaxiang Zhang, Cheng Liang*, Guanghui Li, Qiu Xiao, Pingjian Ding, and Jiawei Luo. “Self-weighted multi-kernel multi-label learning for potential miRNA-disease association prediction.” Molecular Therapy-Nucleic Acids 17 (2019): 414-423. [Link]\nGuanghui Li*, Jiawei Luo, Cheng Liang, Qiu Xiao, Pingjian Ding, and Yuejin Zhang. “Prediction of LncRNA-disease associations based on network consistency projection.” Ieee Access 7 (2019): 58849-58856. [Link]\nShengpeng Yu, Cheng Liang*, Qiu Xiao, Guanghui Li, Pingjian Ding, and JiaWei Luo. “MCLPMDA: A novel method for mi RNA‐disease association prediction based on matrix completion and label propagation.” Journal of cellular and molecular medicine 23, no. 2 (2019): 1427-1438. [Link]\nGuanghui Li*, Yingjie Yue, Cheng Liang, Qiu Xiao, Pingjian Ding, and Jiawei Luo. “NCPCDA: network consistency projection for circRNA–disease association prediction.” RSC advances 9, no. 57 (2019): 33222-33228. [Link]\nPingjian Ding, Rui Yin, Jiawei Luo*, and Chee-Keong Kwoh. “Ensemble prediction of synergistic drug combinations incorporating biological, chemical, pharmacological, and network knowledge.” IEEE journal of biomedical and health informatics 23, no. 3 (2019): 1336-1345.[Link]\nYing Liu, Jiawei Luo*, and Pingjian Ding. “Inferring microRNA targets based on restricted Boltzmann machines.” IEEE journal of biomedical and health informatics 23, no. 1 (2019): 427-436.[Link]\n\n\n\n\n2018\n\nShengpeng Yu, Cheng Liang*, Qiu Xiao, Guanghui Li, Pingjian Ding, and Jiawei Luo. “GLNMDA: a novel method for miRNA-disease association prediction based on global linear neighborhoods.” RNA biology 15, no. 9 (2018): 1215-1227. [Link]\nXiao, Qiu, Jiawei Luo*, Cheng Liang, Guanghui Li, Jie Cai, Pingjian Ding, and Ying Liu. “Identifying lncRNA and mRNA co-expression modules from matched expression data in ovarian cancer.” IEEE/ACM transactions on computational biology and bioinformatics 17, no. 2 (2018): 623-634. [Link]\nYu Qu, Huaxiang Zhang, Cheng Liang*, Pingjian Ding, and Jiawei Luo. “SNMDA: A novel method for predicting micro RNA‐disease associations based on sparse neighbourhood.” Journal of Cellular and Molecular Medicine 22, no. 10 (2018): 5109-5120. [Link]\nBuwen Cao*, Shuguang Deng, Hua Qin, Pingjian Ding, Shaopeng Chen, and Guanghui Li. “Detection of protein complexes based on penalized matrix decomposition in a sparse protein–protein interaction network.” Molecules 23, no. 6 (2018): 1460. [Link]\nJiawei Luo*, Pingjian Ding, Cheng Liang, and Xiangtao Chen. “Semi-supervised prediction of human miRNA-disease association based on graph regularization framework in heterogeneous networks.” Neurocomputing 294 (2018): 29-38. [Link]\nGuanghui Li*, Jiawei Luo, Qiu Xiao, Cheng Liang, and Pingjian Ding. “Predicting microRNA-disease associations using label propagation based on linear neighborhood similarity.” Journal of biomedical informatics 82 (2018): 169-177. [Link]\nPingjian Ding, Jiawei Luo*, Cheng Liang, Qiu Xiao, Buwen Cao, and Guanghui Li. “Discovering synergistic drug combination from a computational perspective.” Current Topics in Medicinal Chemistry 18, no. 12 (2018): 965-974. [Link]\nPingjian Ding, Jiawei Luo*, Cheng Liang, Qiu Xiao, and Buwen Cao. “Human disease MiRNA inference by combining target information based on heterogeneous manifolds.” Journal of biomedical informatics 80 (2018): 26-36. [Link]\nQiu Xiao, Jiawei Luo*, Cheng Liang, Jie Cai, and Pingjian Ding. “A graph regularized non-negative matrix factorization method for identifying microRNA-disease associations.” Bioinformatics 34, no. 2 (2018): 239-248. [Link]\nQiao Zhu, Jiawei Luo*, Pingjian Ding, and Qiu Xiao. “GRTR: Drug-disease association prediction based on graph regularized transductive regression on heterogeneous network.” In Bioinformatics Research and Applications: 14th International Symposium, ISBRA 2018, Beijing, China, June 8-11, 2018, Proceedings 14, pp. 13-25. Springer International Publishing, 2018. [Link]\nGuanghui Li, Jiawei Luo*, Qiu Xiao, Cheng Liang, and Pingjian Ding. “Prediction of microRNA–disease associations with a Kronecker kernel matrix dimension reduction model.” RSC advances 8, no. 8 (2018): 4377-4385. [Link]\n\n\n\n\n2017\n\nGuanghui Li, Jiawei Luo*, Qiu Xiao, Cheng Liang, Pingjian Ding, and Buwen Cao. “Predicting microrna-disease associations using network topological similarity based on deepwalk.” Ieee Access 5 (2017): 24032-24039. [Link]\nJiawei Luo*, Qiu Xiao, Cheng Liang, and Pingjian Ding. “Predicting MicroRNA-disease associations using Kronecker regularized least squares based on heterogeneous omics data.” Ieee Access 5 (2017): 2503-2513. [Link]\nPingjian Ding, Jiawei Luo*, Cheng Liang, Jie Cai, Ying Liu, and Xiangtao Chen. “A novel group wise-based method for calculating human miRNA functional similarity.” IEEE Access 5 (2017): 2364-2372. [Link]\nBuwen Cao, Shuguang Deng, Jiawei Luo*, Pingjian Ding, and Shulin Wang. “Identification of overlapping protein complexes by fuzzy K-medoids clustering algorithm in yeast protein-protein interaction networks.” Journal of Intelligent & Fuzzy Systems 34, no. 1 (2018): 93-103. [Link]\nJiawei Luo*, Pingjian Ding, Cheng Liang, Buwen Cao, and Xiangtao Chen. “Collective prediction of disease-associated miRNAs based on transduction learning.” IEEE/ACM transactions on computational biology and bioinformatics 14, no. 6 (2017): 1468-1475. [Link]\n\n\n\n\n2016\n\nBuwen Cao, Jiawei Luo*, Cheng Liang, Shulin Wang, and Pingjian Ding. “Pce-fr: A novel method for identifying overlapping protein complexes in weighted protein-protein interaction networks using pseudo-clique extension based on fuzzy relation.” IEEE transactions on nanobioscience 15, no. 7 (2016): 728-738. [Link]\nPingjian Ding, Jiawei Luo*, Qiu Xiao, and Xiangtao Chen. “A path-based measurement for human miRNA functional similarities using miRNA-disease associations.” Scientific Reports 6, no. 1 (2016): 32533. [Link]\nJiawei Luo*, Cong Huang, and Pingjian Ding. “A meta-path-based prediction method for human miRNA-target association.” BioMed Research International 2016 (2016). [Link]"
  }
]